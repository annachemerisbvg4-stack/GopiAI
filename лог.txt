
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
   ü§ñ CrewAI API Server Environment
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üîÑ Activating CrewAI environment...
‚úÖ Environment activated
üìÇ Directory: GopiAI-CrewAI
üêç Environment: C:\Users\crazy\GOPI_AI_MODULES\rag_memory_env

üöÄ Starting CrewAI API Server...
DEBUG: Content of llm_rotation_config.py at C:\Users\crazy\GOPI_AI_MODULES\GopiAI-CrewAI\llm_rotation_config.py:
import os
import time
import threading
# –ö–æ–Ω—Ñ–∏–≥ –º–æ–¥–µ–ª–µ–π Gemini/Gemma –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏ –∏ –∑–∞–¥–∞—á
LLM_MODELS_CONFIG = [
    {
        "name": "Gemma 3",
        "id": "gemini/gemma-3",
        "provider": "google",
        "rpm": 2,  # –î–ï–ú–û: —Å–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–∏–ª–∏
        "tpm": 100,  # –î–ï–ú–û: —Å–∏–ª—å–Ω–æ —É–º–µ–Ω—å—à–∏–ª–∏
        "type": ["simple", "lookup", "short_answer"],
        "multimodal": False,
        "embedding": False,
        "priority": 1,
        "rpd": 0,
        "deprecated": False,
        "base_score": 0...
‚Üê[92m00:10:26 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:533 - Using AiohttpTransport...
2025-07-08 00:10:26,421 - LiteLLM - DEBUG - Using AiohttpTransport...
‚Üê[92m00:10:26 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:557 - Creating AiohttpTransport...
2025-07-08 00:10:26,421 - LiteLLM - DEBUG - Creating AiohttpTransport...
2025-07-08 00:10:27,384 - httpcore.connection - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-07-08 00:10:27,458 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002301963C3E0>
2025-07-08 00:10:27,459 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002301972E8D0> server_hostname='raw.githubusercontent.com' timeout=5
2025-07-08 00:10:27,505 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000230194F9DF0>
2025-07-08 00:10:27,507 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-07-08 00:10:27,508 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-07-08 00:10:27,512 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-07-08 00:10:27,514 - httpcore.http11 - DEBUG - send_request_body.complete
2025-07-08 00:10:27,515 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-07-08 00:10:27,542 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'26277'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"2e2e8fe8b4766044ddbd63788232b654be39bca74aa232fe7f6ddefb92cb4234"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8C23:0906:517CF:E1F51:686952CD'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 07 Jul 2025 18:40:22 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-del21741-DEL'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'3'), (b'X-Timer', b'S1751913623.762620,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'07695c5d77d53142ee4ada1fcbeed93c82783a52'), (b'Expires', b'Mon, 07 Jul 2025 18:45:22 GMT'), (b'Source-Age', b'259')])
2025-07-08 00:10:27,550 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-08 00:10:27,552 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-07-08 00:10:27,556 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-07-08 00:10:27,557 - httpcore.http11 - DEBUG - response_closed.started
2025-07-08 00:10:27,558 - httpcore.http11 - DEBUG - response_closed.complete
2025-07-08 00:10:27,560 - httpcore.connection - DEBUG - close.started
2025-07-08 00:10:27,561 - httpcore.connection - DEBUG - close.complete
‚Üê[92m00:10:28 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:533 - Using AiohttpTransport...
2025-07-08 00:10:28,124 - LiteLLM - DEBUG - Using AiohttpTransport...
‚Üê[92m00:10:28 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:557 - Creating AiohttpTransport...
2025-07-08 00:10:28,127 - LiteLLM - DEBUG - Creating AiohttpTransport...
‚Üê[92m00:10:28 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:169 - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2025-07-08 00:10:28,534 - LiteLLM - DEBUG - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
‚Üê[92m00:10:28 - LiteLLM:DEBUG‚Üê[0m: transformation.py:17 - [Non-Blocking] Unable to import _ENTERPRISE_ResponsesSessionHandler - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2025-07-08 00:10:28,898 - LiteLLM - DEBUG - [Non-Blocking] Unable to import _ENTERPRISE_ResponsesSessionHandler - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
‚Üê[92m00:10:28 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:533 - Using AiohttpTransport...
2025-07-08 00:10:28,934 - LiteLLM - DEBUG - Using AiohttpTransport...
‚Üê[92m00:10:28 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:557 - Creating AiohttpTransport...
2025-07-08 00:10:28,959 - LiteLLM - DEBUG - Creating AiohttpTransport...
‚Üê[92m00:10:28 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:533 - Using AiohttpTransport...
2025-07-08 00:10:28,993 - LiteLLM - DEBUG - Using AiohttpTransport...
‚Üê[92m00:10:29 - LiteLLM:DEBUG‚Üê[0m: http_handler.py:557 - Creating AiohttpTransport...
2025-07-08 00:10:29,005 - LiteLLM - DEBUG - Creating AiohttpTransport...
DEBUG: LLM_MODELS_CONFIG loaded: [{'name': 'Gemma 3', 'id': 'gemini/gemma-3', 'provider': 'google', 'rpm': 2, 'tpm': 100, 'type': ['simple', 'lookup', 'short_answer'], 'multimodal': False, 'embedding': False, 'priority': 1, 'rpd': 0, 'deprecated': False, 'base_score': 0.5}, {'name': 'Gemma 3n', 'id': 'gemini/gemma-3n', 'provider': 'google', 'rpm': 2, 'tpm': 200, 'type': ['simple', 'lookup', 'short_answer'], 'multimodal': False, 'embedding': False, 'priority': 2, 'rpd': 0, 'deprecated': False, 'base_score': 0.5}, {'name': 'Gemini 1.5 Flash', 'id': 'gemini/gemini-1.5-flash', 'provider': 'google', 'rpm': 15, 'tpm': 250000, 'type': ['simple', 'dialog', 'code', 'summarize'], 'multimodal': False, 'embedding': False, 'priority': 3, 'rpd': 50, 'deprecated': False, 'base_score': 0.5}, {'name': 'Gemini 2.0 Flash-Lite', 'id': 'gemini/gemini-2.0-flash-lite', 'provider': 'google', 'rpm': 30, 'tpm': 1000000, 'type': ['simple', 'dialog', 'code', 'summarize'], 'multimodal': False, 'embedding': False, 'priority': 4, 'rpd': 200, 'deprecated': False, 'base_score': 0.5}, {'name': 'Gemini 2.5 Flash-Lite Preview', 'id': 'gemini/gemini-2.5-flash-lite-preview', 'provider': 'google', 'rpm': 15, 'tpm': 60000, 'type': ['dialog', 'code', 'summarize'], 'multimodal': False, 'embedding': False, 'priority': 5, 'rpd': 0, 'deprecated': False, 'base_score': 0.5}, {'name': 'Gemini 2.5 Flash', 'id': 'gemini/gemini-2.5-flash', 'provider': 'google', 'rpm': 10, 'tpm': 60000, 'type': ['dialog', 'code', 'multimodal', 'vision', 'long_answer'], 'multimodal': True, 'embedding': False, 'priority': 6, 'rpd': 0, 'deprecated': False, 'base_score': 0.5}, {'name': 'Gemini Embedding Experimental', 'id': 'gemini/gemini-embedding-experimental', 'provider': 'google', 'rpm': 5, 'tpm': 10000, 'type': ['embedding'], 'multimodal': False, 'embedding': True, 'priority': 10, 'rpd': 0, 'deprecated': False, 'base_score': 0.5}]
[OK] RateLimitMonitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å blacklist –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
[OK] –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LLM –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ
GopiAI Integration Tools loaded successfully!
‚úÖ CrewAI —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω!
2025-07-08 00:10:30,688 - tools.gopiai_integration.ai_router_llm - INFO - ‚úÖ AIRouterLLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π —Ä–æ—Ç–∞—Ü–∏–∏
2025-07-08 00:10:30,688 - tools.gopiai_integration.self_reflection - INFO - ReflectionEnabledAIRouter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (reflection=enabled)
2025-07-08 00:10:30,688 - tools.gopiai_integration.self_reflection - INFO - –ü–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞: 8.0/10
‚úÖ AI Router —Å —Å–∞–º–æ—Ä–µ—Ñ–ª–µ–∫—Å–∏–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω (–ø–æ—Ä–æ–≥ –∫–∞—á–µ—Å—Ç–≤–∞: 8.0)
2025-07-08 00:10:30,705 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): 127.0.0.1:5051
‚ö†Ô∏è RAG-—Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª—è—Ç—å—Å—è.
2025-07-08 00:10:32,708 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): 127.0.0.1:5051
Smart Delegator —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω
Starting CrewAI API server at http://127.0.0.1:5050
This server should be run from the CrewAI environment (crewai_env)
Using Python: C:\Users\crazy\GOPI_AI_MODULES\rag_memory_env\Scripts\python.exe
CrewAI module is available
Langchain module is available
Request timeout: 300 seconds
Connection timeout: 60 seconds
 * Serving Flask app 'crewai_api_server'
 * Debug mode: off
2025-07-08 00:10:34,746 - werkzeug - INFO - ‚Üê[31m‚Üê[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.‚Üê[0m
 * Running on http://127.0.0.1:5050
2025-07-08 00:10:34,746 - werkzeug - INFO - ‚Üê[33mPress CTRL+C to quit‚Üê[0m
2025-07-08 00:10:57,833 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): 127.0.0.1:5051
2025-07-08 00:10:59,848 - werkzeug - INFO - 127.0.0.1 - - [08/Jul/2025 00:10:59] "GET /api/health HTTP/1.1" 200 -
üìä –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: —Å–ª–æ–∂–Ω–æ—Å—Ç—å=0, —Ç–∏–ø=general, CrewAI=False
üìä –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: —Å–ª–æ–∂–Ω–æ—Å—Ç—å=0, —Ç–∏–ø=general, CrewAI=False
üìä –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: —Å–ª–æ–∂–Ω–æ—Å—Ç—å=0, —Ç–∏–ø=general, CrewAI=False
üìä –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: —Å–ª–æ–∂–Ω–æ—Å—Ç—å=0, —Ç–∏–ø=general, CrewAI=False
2025-07-08 00:11:09,191 - tools.gopiai_integration.smart_delegator - INFO - üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ AI Router...
2025-07-08 00:11:09,192 - tools.gopiai_integration.smart_delegator - INFO - ‚úÖ AI Router –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã
2025-07-08 00:11:09,193 - tools.gopiai_integration.smart_delegator - INFO - üöÄ –í—ã–∑–æ–≤ AI Router...
2025-07-08 00:11:09,194 - tools.gopiai_integration.ai_router_llm - INFO - üß† –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ 1/1
2025-07-08 00:11:09,195 - tools.gopiai_integration.ai_router_llm - INFO - üß† –û–±—ã—á–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞
[AVAILABLE] –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è 'dialog': ['gemini/gemini-1.5-flash', 'gemini/gemini-2.0-flash-lite', 'gemini/gemini-2.5-flash-lite-preview', 'gemini/gemini-2.5-flash']
[CHECK] –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ gemini/gemini-1.5-flash —Å 2 —Ç–æ–∫–µ–Ω–∞–º–∏...
[STATS] –ú–æ–¥–µ–ª—å gemini/gemini-1.5-flash: —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RPM=0/15, TPM=0/250000, RPD=0/50
[OK] –ú–æ–¥–µ–ª—å gemini/gemini-1.5-flash: RPM_OK=True, TPM_OK=True, RPD_OK=True -> RESULT=True
[SELECTED] AI Router: –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å 'gemini/gemini-1.5-flash' (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3)
2025-07-08 00:11:09,201 - tools.gopiai_integration.ai_router_llm - INFO - üéØ –ü–æ–ø—ã—Ç–∫–∞ 1: –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å 'gemini/gemini-1.5-flash' (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: 3, score: 0.5)
[DEBUG] Getting API key for provider: google
[DEBUG] Environment test suffix: ''
[DEBUG] Key map: {'google': 'GEMINI_API_KEY'}
[DEBUG] Environment variable base for google: 'GEMINI_API_KEY'
[DEBUG] Full environment variable name: 'GEMINI_API_KEY'
[DEBUG] Successfully retrieved API key for google
[DEBUG] API key starts with: AIzaS...Lgu0Q
2025-07-08 00:11:09,213 - tools.gopiai_integration.ai_router_llm - INFO - üîÑ –ü–æ–ø—ã—Ç–∫–∞ 1: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ gemini/gemini-1.5-flash
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:340 -

2025-07-08 00:11:09,216 - LiteLLM - DEBUG -

‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:340 - ‚Üê[92mRequest to litellm:‚Üê[0m
2025-07-08 00:11:09,219 - LiteLLM - DEBUG - ‚Üê[92mRequest to litellm:‚Üê[0m
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:340 - ‚Üê[92mlitellm.completion(model='gemini/gemini-1.5-flash', messages=[{'role': 'user', 'content': '–ø—Ä–∏–≤–µ—Ç'}], stop=[], api_key='AIzaSyD2taiOJ6Z7nLcVQrz_U0ct4G0HlyLgu0Q', stream=False, config={'temperature': 0.7, 'max_tokens': 2000})‚Üê[0m
2025-07-08 00:11:09,224 - LiteLLM - DEBUG - ‚Üê[92mlitellm.completion(model='gemini/gemini-1.5-flash', messages=[{'role': 'user', 'content': '–ø—Ä–∏–≤–µ—Ç'}], stop=[], api_key='AIzaSyD2taiOJ6Z7nLcVQrz_U0ct4G0HlyLgu0Q', stream=False, config={'temperature': 0.7, 'max_tokens': 2000})‚Üê[0m
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:340 -

2025-07-08 00:11:09,233 - LiteLLM - DEBUG -

‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:461 - self.optional_params: {}
2025-07-08 00:11:09,236 - LiteLLM - DEBUG - self.optional_params: {}
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:340 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
2025-07-08 00:11:09,238 - LiteLLM - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'vertex_ai/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-1.5-flash', 'custom_llm_provider': 'vertex_ai'}
2025-07-08 00:11:09,251 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'vertex_ai/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-1.5-flash', 'custom_llm_provider': 'vertex_ai'}
‚Üê[92m00:11:09 - LiteLLM:INFO‚Üê[0m: utils.py:3119 -
LiteLLM completion() model= gemini-1.5-flash; provider = gemini
2025-07-08 00:11:09,256 - LiteLLM - INFO -
LiteLLM completion() model= gemini-1.5-flash; provider = gemini
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:3122 -
LiteLLM: Params passed to completion() {'model': 'gemini-1.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': [], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '–ø—Ä–∏–≤–µ—Ç'}], 'thinking': None, 'web_search_options': None, 'config': {'temperature': 0.7, 'max_tokens': 2000}}
2025-07-08 00:11:09,259 - LiteLLM - DEBUG -
LiteLLM: Params passed to completion() {'model': 'gemini-1.5-flash', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': [], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'gemini', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '–ø—Ä–∏–≤–µ—Ç'}], 'thinking': None, 'web_search_options': None, 'config': {'temperature': 0.7, 'max_tokens': 2000}}
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:3125 -
LiteLLM: Non-Default params passed to completion() {'stream': False, 'stop': []}
2025-07-08 00:11:09,267 - LiteLLM - DEBUG -
LiteLLM: Non-Default params passed to completion() {'stream': False, 'stop': []}
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:340 - Final returned optional params: {'stop_sequences': [], 'config': {'temperature': 0.7, 'max_tokens': 2000}}
2025-07-08 00:11:09,271 - LiteLLM - DEBUG - Final returned optional params: {'stop_sequences': [], 'config': {'temperature': 0.7, 'max_tokens': 2000}}
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:461 - self.optional_params: {'stream': False, 'stop': [], 'config': {'temperature': 0.7, 'max_tokens': 2000}}
2025-07-08 00:11:09,276 - LiteLLM - DEBUG - self.optional_params: {'stream': False, 'stop': [], 'config': {'temperature': 0.7, 'max_tokens': 2000}}
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:09,279 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:09,640 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:09 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:908 - ‚Üê[92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=*****gu0Q \
-H 'Content-Type: ap****on' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '–ø—Ä–∏–≤–µ—Ç'}]}], 'generationConfig': {'stop_sequences': []}}'
‚Üê[0m

2025-07-08 00:11:09,647 - LiteLLM - DEBUG - ‚Üê[92m

POST Request Sent from LiteLLM:
curl -X POST \
https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=*****gu0Q \
-H 'Content-Type: ap****on' \
-d '{'contents': [{'role': 'user', 'parts': [{'text': '–ø—Ä–∏–≤–µ—Ç'}]}], 'generationConfig': {'stop_sequences': []}}'
‚Üê[0m

2025-07-08 00:11:10,003 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=600.0 socket_options=None
2025-07-08 00:11:10,194 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002302157F2C0>
2025-07-08 00:11:10,194 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000230215B9250> server_hostname='generativelanguage.googleapis.com' timeout=600.0
2025-07-08 00:11:10,237 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002302157F1D0>
2025-07-08 00:11:10,237 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-07-08 00:11:10,237 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-07-08 00:11:10,237 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-07-08 00:11:10,242 - httpcore.http11 - DEBUG - send_request_body.complete
2025-07-08 00:11:10,242 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-07-08 00:11:12,128 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 07 Jul 2025 18:41:07 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1859'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2025-07-08 00:11:12,131 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyD2taiOJ6Z7nLcVQrz_U0ct4G0HlyLgu0Q "HTTP/1.1 200 OK"
2025-07-08 00:11:12,133 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-07-08 00:11:12,135 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-07-08 00:11:12,135 - httpcore.http11 - DEBUG - response_closed.started
2025-07-08 00:11:12,135 - httpcore.http11 - DEBUG - response_closed.complete
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:340 - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.014275607963403067
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 2,
    "candidatesTokenCount": 6,
    "totalTokenCount": 8,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 2
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 6
      }
    ]
  },
  "modelVersion": "gemini-1.5-flash",
  "responseId": "whRsaMDQOsyBmecP7ZKEsAQ"
}



2025-07-08 00:11:12,137 - LiteLLM - DEBUG - RAW RESPONSE:
{
  "candidates": [
    {
      "content": {
        "parts": [
          {
            "text": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?\n"
          }
        ],
        "role": "model"
      },
      "finishReason": "STOP",
      "avgLogprobs": -0.014275607963403067
    }
  ],
  "usageMetadata": {
    "promptTokenCount": 2,
    "candidatesTokenCount": 6,
    "totalTokenCount": 8,
    "promptTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 2
      }
    ],
    "candidatesTokensDetails": [
      {
        "modality": "TEXT",
        "tokenCount": 6
      }
    ]
  },
  "modelVersion": "gemini-1.5-flash",
  "responseId": "whRsaMDQOsyBmecP7ZKEsAQ"
}



2025-07-08 00:11:12,168 - httpcore.connection - DEBUG - close.started
2025-07-08 00:11:12,169 - httpcore.connection - DEBUG - close.complete
‚Üê[92m00:11:12 - LiteLLM:INFO‚Üê[0m: utils.py:1215 - Wrapper: Completed Call, calling success_handler
2025-07-08 00:11:12,170 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:1394 - Logging Details LiteLLM-Success Call: Cache_hit=None
2025-07-08 00:11:12,172 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None
‚Üê[92m00:11:12 - LiteLLM:INFO‚Üê[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-1.5-flash
2025-07-08 00:11:12,173 - LiteLLM - INFO - selected model name for cost calculation: gemini/gemini-1.5-flash
‚Üê[92m00:11:12 - LiteLLM:INFO‚Üê[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-1.5-flash
2025-07-08 00:11:12,174 - LiteLLM - INFO - selected model name for cost calculation: gemini/gemini-1.5-flash
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,181 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,182 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,188 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,191 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4783 - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
2025-07-08 00:11:12,196 - LiteLLM - DEBUG - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4783 - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
2025-07-08 00:11:12,197 - LiteLLM - DEBUG - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:1131 - response_cost: 1.95e-06
2025-07-08 00:11:12,217 - LiteLLM - DEBUG - response_cost: 1.95e-06
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:1131 - response_cost: 1.95e-06
2025-07-08 00:11:12,226 - tools.gopiai_integration.ai_router_llm - INFO - ‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ gemini/gemini-1.5-flash
2025-07-08 00:11:12,226 - LiteLLM - DEBUG - response_cost: 1.95e-06
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'vertex_ai/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-1.5-flash', 'custom_llm_provider': 'vertex_ai'}
2025-07-08 00:11:12,230 - tools.gopiai_integration.ai_router_llm - INFO - ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω –ø—Ä–æ–º–ø—Ç 1 –º–æ–¥–µ–ª—å—é gemini/gemini-1.5-flash
2025-07-08 00:11:12,234 - tools.gopiai_integration.smart_delegator - INFO - ‚úÖ –û—Ç–≤–µ—Ç –æ—Ç AI Router –ø–æ–ª—É—á–µ–Ω
2025-07-08 00:11:12,231 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'vertex_ai/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-1.5-flash', 'custom_llm_provider': 'vertex_ai'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,235 - tools.gopiai_integration.smart_delegator - INFO - ‚è± –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ 3.04 —Å–µ–∫
‚è± –ó–∞–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∑–∞ 3.05 —Å–µ–∫
2025-07-08 00:11:12,236 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω, –¥–ª–∏–Ω–∞: 18 —Å–∏–º–≤–æ–ª–æ–≤
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4783 - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
2025-07-08 00:11:12,245 - werkzeug - INFO - 127.0.0.1 - - [08/Jul/2025 00:11:12] "POST /api/process HTTP/1.1" 200 -
2025-07-08 00:11:12,245 - LiteLLM - DEBUG - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:1423 - Logging Details LiteLLM-Success Call streaming complete
2025-07-08 00:11:12,264 - LiteLLM - DEBUG - Logging Details LiteLLM-Success Call streaming complete
‚Üê[92m00:11:12 - LiteLLM:INFO‚Üê[0m: cost_calculator.py:655 - selected model name for cost calculation: gemini/gemini-1.5-flash
2025-07-08 00:11:12,269 - LiteLLM - INFO - selected model name for cost calculation: gemini/gemini-1.5-flash
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,272 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,277 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4783 - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
2025-07-08 00:11:12,280 - LiteLLM - DEBUG - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: litellm_logging.py:1131 - response_cost: 1.95e-06
2025-07-08 00:11:12,294 - LiteLLM - DEBUG - response_cost: 1.95e-06
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'vertex_ai/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-1.5-flash', 'custom_llm_provider': 'vertex_ai'}
2025-07-08 00:11:12,297 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'vertex_ai/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'vertex_ai/gemini-1.5-flash', 'custom_llm_provider': 'vertex_ai'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4481 - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
2025-07-08 00:11:12,300 - LiteLLM - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'gemini-1.5-flash', 'combined_model_name': 'gemini/gemini-1.5-flash', 'stripped_model_name': 'gemini-1.5-flash', 'combined_stripped_model_name': 'gemini/gemini-1.5-flash', 'custom_llm_provider': 'gemini'}
‚Üê[92m00:11:12 - LiteLLM:DEBUG‚Üê[0m: utils.py:4783 - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}
2025-07-08 00:11:12,305 - LiteLLM - DEBUG - model_info: {'key': 'gemini/gemini-1.5-flash', 'max_tokens': 8192, 'max_input_tokens': 1048576, 'max_output_tokens': 8192, 'input_cost_per_token': 7.5e-08, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': 1.5e-07, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': 6e-07, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'gemini', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': None, 'supports_prompt_caching': None, 'supports_audio_input': None, 'supports_audio_output': None, 'supports_pdf_input': None, 'supports_embedding_image_input': None, 'supports_native_streaming': None, 'supports_web_search': None, 'supports_url_context': None, 'supports_reasoning': None, 'supports_computer_use': None, 'search_context_cost_per_query': None, 'tpm': 4000000, 'rpm': 2000}







‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
   üñ•Ô∏è GopiAI-UI Application
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üîÑ Activating UI environment...
‚úÖ Environment activated
üìÇ Directory: GopiAI-UI
üêç Environment: C:\Users\crazy\GOPI_AI_MODULES\rag_memory_env

üöÄ Starting GopiAI-UI Application...
2025-07-08 00:10:32,016 - GopiAI - INFO - üéØ GopiAI Unified Logger initialized
üîß DEBUG logging enabled for crewai_client.py
2025-07-08 00:10:37,153 - pymorphy3.opencorpora_dict.wrapper - INFO - Loading dictionaries from C:\Users\crazy\GOPI_AI_MODULES\rag_memory_env\Lib\site-packages\pymorphy3_dicts_ru\data
2025-07-08 00:10:37,193 - pymorphy3.opencorpora_dict.wrapper - INFO - format: 2.4, revision: 417150, updated: 2022-01-08T22:09:24.565962
2025-07-08 00:10:39,120 - gopiai.ui.components.crewai_client - INFO - ‚úÖ spaCy –∏ —è–∑—ã–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã
2025-07-08 00:10:39,127 - gopiai.ui.components.chat_widget - INFO - ‚úÖ RAG context function defined directly
üîß Enhanced DEBUG logging enabled for chat_widget.py (console + file)
2025-07-08 00:10:39,130 - gopiai.ui.components.chat_widget - INFO - === Chat Widget Debug Session Started ===
–ú–æ–¥—É–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è GopiAI v0.3.2 —Å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π —Ç–µ–º
–î–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è –º–æ–¥—É–ª–µ–π:
- C:\Users\crazy\GOPI_AI_MODULES\GopiAI-Core (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True)
- C:\Users\crazy\GOPI_AI_MODULES\GopiAI-Widgets (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True)
- C:\Users\crazy\GOPI_AI_MODULES\GopiAI-App (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True)
- C:\Users\crazy\GOPI_AI_MODULES\GopiAI-Extensions (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True)
- C:\Users\crazy\GOPI_AI_MODULES\rag_memory_system (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True)
- C:\Users\crazy\GOPI_AI_MODULES (—Å—É—â–µ—Å—Ç–≤—É–µ—Ç: True)
[OK] –í—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥—É–ª–∏ UI –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ
2025-07-08 00:10:39,268 - faiss.loader - DEBUG - Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU
2025-07-08 00:10:39,272 - faiss.loader - INFO - Loading faiss with AVX2 support.
2025-07-08 00:10:39,325 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.
2025-07-08 00:10:39,336 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
00:10:49 - LiteLLM:DEBUG: http_handler.py:530 - Using AiohttpTransport...
2025-07-08 00:10:49,312 - LiteLLM - DEBUG - Using AiohttpTransport...
00:10:49 - LiteLLM:DEBUG: http_handler.py:554 - Creating AiohttpTransport...
2025-07-08 00:10:49,317 - LiteLLM - DEBUG - Creating AiohttpTransport...
2025-07-08 00:10:50,296 - httpcore.connection - DEBUG - connect_tcp.started host='raw.githubusercontent.com' port=443 local_address=None timeout=5 socket_options=None
2025-07-08 00:10:50,358 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000018B73377AD0>
2025-07-08 00:10:50,360 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000018B733524D0> server_hostname='raw.githubusercontent.com' timeout=5
2025-07-08 00:10:50,405 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000018B73377A10>
2025-07-08 00:10:50,407 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-07-08 00:10:50,409 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-07-08 00:10:50,413 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-07-08 00:10:50,415 - httpcore.http11 - DEBUG - send_request_body.complete
2025-07-08 00:10:50,416 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-07-08 00:10:50,443 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Connection', b'keep-alive'), (b'Content-Length', b'26277'), (b'Cache-Control', b'max-age=300'), (b'Content-Security-Policy', b"default-src 'none'; style-src 'unsafe-inline'; sandbox"), (b'Content-Type', b'text/plain; charset=utf-8'), (b'ETag', b'W/"2e2e8fe8b4766044ddbd63788232b654be39bca74aa232fe7f6ddefb92cb4234"'), (b'Strict-Transport-Security', b'max-age=31536000'), (b'X-Content-Type-Options', b'nosniff'), (b'X-Frame-Options', b'deny'), (b'X-XSS-Protection', b'1; mode=block'), (b'X-GitHub-Request-Id', b'8C23:0906:517CF:E1F51:686952CD'), (b'Content-Encoding', b'gzip'), (b'Accept-Ranges', b'bytes'), (b'Date', b'Mon, 07 Jul 2025 18:40:45 GMT'), (b'Via', b'1.1 varnish'), (b'X-Served-By', b'cache-del21738-DEL'), (b'X-Cache', b'HIT'), (b'X-Cache-Hits', b'5'), (b'X-Timer', b'S1751913646.663167,VS0,VE0'), (b'Vary', b'Authorization,Accept-Encoding'), (b'Access-Control-Allow-Origin', b'*'), (b'Cross-Origin-Resource-Policy', b'cross-origin'), (b'X-Fastly-Request-ID', b'712e630aa280a30ebe2f6ac4859d49c13a75076b'), (b'Expires', b'Mon, 07 Jul 2025 18:45:45 GMT'), (b'Source-Age', b'282')])
2025-07-08 00:10:50,452 - httpx - INFO - HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json "HTTP/1.1 200 OK"
2025-07-08 00:10:50,455 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-07-08 00:10:50,460 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-07-08 00:10:50,462 - httpcore.http11 - DEBUG - response_closed.started
2025-07-08 00:10:50,463 - httpcore.http11 - DEBUG - response_closed.complete
2025-07-08 00:10:50,464 - httpcore.connection - DEBUG - close.started
2025-07-08 00:10:50,466 - httpcore.connection - DEBUG - close.complete
00:10:52 - LiteLLM:DEBUG: http_handler.py:530 - Using AiohttpTransport...
2025-07-08 00:10:52,471 - LiteLLM - DEBUG - Using AiohttpTransport...
00:10:52 - LiteLLM:DEBUG: http_handler.py:554 - Creating AiohttpTransport...
2025-07-08 00:10:52,471 - LiteLLM - DEBUG - Creating AiohttpTransport...
00:10:52 - LiteLLM:DEBUG: litellm_logging.py:168 - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2025-07-08 00:10:52,884 - LiteLLM - DEBUG - [Non-Blocking] Unable to import GenericAPILogger - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
00:10:53 - LiteLLM:DEBUG: transformation.py:17 - [Non-Blocking] Unable to import _ENTERPRISE_ResponsesSessionHandler - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
2025-07-08 00:10:53,262 - LiteLLM - DEBUG - [Non-Blocking] Unable to import _ENTERPRISE_ResponsesSessionHandler - LiteLLM Enterprise Feature - No module named 'litellm_enterprise'
00:10:53 - LiteLLM:DEBUG: http_handler.py:530 - Using AiohttpTransport...
2025-07-08 00:10:53,278 - LiteLLM - DEBUG - Using AiohttpTransport...
00:10:53 - LiteLLM:DEBUG: http_handler.py:554 - Creating AiohttpTransport...
2025-07-08 00:10:53,278 - LiteLLM - DEBUG - Creating AiohttpTransport...
00:10:53 - LiteLLM:DEBUG: http_handler.py:530 - Using AiohttpTransport...
2025-07-08 00:10:53,292 - LiteLLM - DEBUG - Using AiohttpTransport...
00:10:53 - LiteLLM:DEBUG: http_handler.py:554 - Creating AiohttpTransport...
2025-07-08 00:10:53,292 - LiteLLM - DEBUG - Creating AiohttpTransport...
2025-07-08 00:10:53,683 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-07-08 00:10:54,078 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-07-08 00:10:54,113 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2025-07-08 00:10:54,398 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-07-08 00:10:54,445 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2025-07-08 00:10:54,727 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2025-07-08 00:10:54,764 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2025-07-08 00:10:55,042 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-07-08 00:10:55,085 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2025-07-08 00:10:56,938 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-07-08 00:10:56,985 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2025-07-08 00:10:57,278 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
‚úÖ txtai –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
üìä –í–µ—Ä—Å–∏—è txtai: –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ
2025-07-08 00:10:57,393 - gopiai.ui.memory_initializer - INFO - ‚úÖ txtai –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
[MEMORY] Initialisation status: True
[LAUNCH] –ó–∞–ø—É—Å–∫ –º–æ–¥—É–ª—å–Ω–æ–≥–æ GopiAI...
üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ GopiAI —Å —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π —Ç–µ–º...
[OK] –°–∏—Å—Ç–µ–º–∞ –∏–∫–æ–Ω–æ–∫ SimpleIconManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
2025-07-08 00:10:57,669 - gopiai.ui.utils.theme_manager - INFO - –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Ç–µ–º–∞: Mint Frost (dark)
‚úÖ –ú–µ–Ω–µ–¥–∂–µ—Ä —Ç–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
2025-07-08 00:10:57,669 - gopiai.ui.utils.theme_manager - INFO - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ–º—ã –∫ QApplication/QCoreApplication
‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–∞ —Ç–µ–º–∞ –∏–∑ —Ñ–∞–π–ª–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫
[SETUP] –ù–∞—Å—Ç—Ä–æ–π–∫–∞ UI –∏–∑ –º–æ–¥—É–ª–µ–π...
INFO: ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
2025-07-08 00:10:57,678 - icon_manager - INFO - ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
WARNING: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
2025-07-08 00:10:57,678 - icon_manager - WARNING - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
‚úÖ Titlebar: –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∏–∫–æ–Ω–æ–∫ UniversalIconManager
INFO: ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
2025-07-08 00:10:57,678 - icon_manager - INFO - ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
WARNING: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
2025-07-08 00:10:57,691 - icon_manager - WARNING - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
‚úÖ Titlebar: –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∏–∫–æ–Ω–æ–∫ UniversalIconManager
INFO: ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
2025-07-08 00:10:57,710 - icon_manager - INFO - ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
WARNING: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
2025-07-08 00:10:57,710 - icon_manager - WARNING - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
INFO: ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
2025-07-08 00:10:57,739 - icon_manager - INFO - ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
WARNING: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
2025-07-08 00:10:57,739 - icon_manager - WARNING - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∏–∫–æ–Ω–æ–∫ UniversalIconManager
üé® CustomFileSystemModel –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å icon_manager: <class 'gopiai.ui.components.icon_file_system_model.UniversalIconManager'>
INFO: ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
2025-07-08 00:10:57,795 - icon_manager - INFO - ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
WARNING: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
2025-07-08 00:10:57,795 - icon_manager - WARNING - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
[OK] Terminal: –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∏–∫–æ–Ω–æ–∫ UniversalIconManager
INFO: ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
2025-07-08 00:10:57,812 - icon_manager - INFO - ‚úÖ Lucide –∏–∫–æ–Ω–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ UniversalIconManager
WARNING: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
2025-07-08 00:10:57,812 - icon_manager - WARNING - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω UniversalIconManager –±–µ–∑ Lucide –∏–∫–æ–Ω–æ–∫
2025-07-08 00:10:57,833 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): 127.0.0.1:5050
2025-07-08 00:10:59,848 - urllib3.connectionpool - DEBUG - http://127.0.0.1:5050 "GET /api/health HTTP/1.1" 200 99
2025-07-08 00:10:59,848 - gopiai.ui.components.chat_widget - INFO - ‚úÖ CrewAI API server is available.
FAISS –¥–æ—Å—Ç—É–ø–µ–Ω
üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è txtai embeddings...
2025-07-08 00:11:00,156 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/nli-mpnet-base-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-07-08 00:11:00,187 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/nli-mpnet-base-v2/20bdd570b61882341b3cedf7fe1246b109f26517/config.json HTTP/1.1" 200 0
2025-07-08 00:11:00,469 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/nli-mpnet-base-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-07-08 00:11:00,513 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/nli-mpnet-base-v2/20bdd570b61882341b3cedf7fe1246b109f26517/config.json HTTP/1.1" 200 0
2025-07-08 00:11:00,798 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/nli-mpnet-base-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2025-07-08 00:11:00,830 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/nli-mpnet-base-v2/20bdd570b61882341b3cedf7fe1246b109f26517/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2025-07-08 00:11:01,100 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/nli-mpnet-base-v2/resolve/main/config.json HTTP/1.1" 307 0
2025-07-08 00:11:01,133 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/nli-mpnet-base-v2/20bdd570b61882341b3cedf7fe1246b109f26517/config.json HTTP/1.1" 200 0
2025-07-08 00:11:01,530 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/nli-mpnet-base-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2025-07-08 00:11:01,569 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/nli-mpnet-base-v2/20bdd570b61882341b3cedf7fe1246b109f26517/tokenizer_config.json HTTP/1.1" 200 0
2025-07-08 00:11:01,880 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/nli-mpnet-base-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
‚úÖ txtai –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω
üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 126 —Å–æ–æ–±—â–µ–Ω–∏–π
üìÅ –§–∞–π–ª—ã –≤–µ–∫—Ç–æ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –æ—Å—Ç–∞–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –ø—É—Å—Ç—ã–º –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–∑–∂–µ
‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Ä–∞–Ω–µ–µ (–Ω–∞–π–¥–µ–Ω —Ñ–ª–∞–≥)
2025-07-08 00:11:01,969 - gopiai.ui.components.chat_widget - INFO - ‚úÖ Embedded memory system available. Stats: {'total_messages': 126, 'total_sessions': 1, 'txtai_available': True, 'vector_messages': 0, 'faiss_available': True, 'data_dir': 'conversations'}
üîç ChatWidget —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ
üîç –ü–µ—Ä–µ–¥–∞–µ–º theme_manager –≤ ChatWidget...
üîç theme_manager –ø–µ—Ä–µ–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ
[OK] –ü–æ–≤–µ–¥–µ–Ω–∏–µ —Å–ø–ª–∏—Ç—Ç–µ—Ä–æ–≤ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
[OK] –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–ø–ª–∏—Ç—Ç–µ—Ä–æ–≤ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
[OK] –ú–æ–¥—É–ª—å–Ω—ã–π UI –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–∞–Ω–µ–ª–µ–π
[OK] –°–∏–≥–Ω–∞–ª openSettingsRequested –ø–æ–¥–∫–ª—é—á–µ–Ω
‚úÖ –°–∏–≥–Ω–∞–ª openBrowserRequested –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ add_browser_tab
‚úÖ –°–∏–≥–Ω–∞–ª—ã –º–µ–Ω—é –ø–æ–¥–∫–ª—é—á–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ
[OK] –ü—Ä–∏–º–µ–Ω–µ–Ω –º–∞–∫–µ—Ç –≤ —Å—Ç–∏–ª–µ VSCode —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º–∏ —Ü–≤–µ—Ç–∞–º–∏
[OK] –ì–æ—Ä—è—á–∏–µ –∫–ª–∞–≤–∏—à–∏ –¥–ª—è –ø–∞–Ω–µ–ª–µ–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã (–≤–∫–ª—é—á–∞—è —Å–±—Ä–æ—Å —Ä–∞–∑–º–µ—Ä–æ–≤)
[OK] –°–∏–≥–Ω–∞–ª file_double_clicked –ø–æ–¥–∫–ª—é—á–µ–Ω
[OK] –°–∏–≥–Ω–∞–ª file_selected –ø–æ–¥–∫–ª—é—á–µ–Ω
[OK] –°–∏–≥–Ω–∞–ª—ã —Ñ–∞–π–ª–æ–≤–æ–≥–æ –ø—Ä–æ–≤–æ–¥–Ω–∏–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã
[OK] –ú–æ–¥—É–ª—å–Ω—ã–π UI –Ω–∞—Å—Ç—Ä–æ–µ–Ω
[OK] FramelessGopiAIStandaloneWindow –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!
üîÑ –ò–∫–æ–Ω–∫–∏ –º–µ–Ω—é –æ–±–Ω–æ–≤–ª–µ–Ω—ã
[SUCCESS] GopiAI v0.3.0 —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω!
[INFO] –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∞–∫—Ç–∏–≤–Ω–∞
[INFO] –†–∞–∑–º–µ—Ä –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–º–µ–Ω—å—à–µ–Ω
2025-07-08 00:11:02,428 - gopiai.ui.utils.theme_manager - INFO - –¢–µ–º–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
2025-07-08 00:11:09,161 - gopiai.ui.components.chat_widget - INFO - append_message: author=–í—ã, text_len=6
2025-07-08 00:11:09,167 - gopiai.ui.components.chat_widget - INFO - append_message: author=–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç, text_len=23
2025-07-08 00:11:09,178 - gopiai.ui.components.chat_widget - INFO - [DEBUG] –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: –ø—Ä–∏–≤–µ—Ç...
2025-07-08 00:11:09,179 - gopiai.ui.components.chat_widget - INFO - [DEBUG] –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ CrewAI –∫–ª–∏–µ–Ω—Ç...
2025-07-08 00:11:09,182 - urllib3.connectionpool - DEBUG - Starting new HTTP connection (1): 127.0.0.1:5050
2025-07-08 00:11:12,252 - urllib3.connectionpool - DEBUG - http://127.0.0.1:5050 "POST /api/process HTTP/1.1" 200 219
2025-07-08 00:11:12,253 - gopiai.ui.components.chat_widget - INFO - [DEBUG] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç CrewAI –∫–ª–∏–µ–Ω—Ç–∞. –¢–∏–ø: <class 'dict'>
2025-07-08 00:11:12,256 - gopiai.ui.components.chat_widget - INFO - [DEBUG] –û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫: {'response': '–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?\n', 'processed_with_crewai': False}...
2025-07-08 00:11:12,258 - gopiai.ui.components.chat_widget - INFO - [DEBUG] –°–∏–≥–Ω–∞–ª response_ready —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω
2025-07-08 00:11:12,259 - gopiai.ui.components.chat_widget - INFO - üîÑ [SIGNAL_HANDLER] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ:
2025-07-08 00:11:12,262 - gopiai.ui.components.chat_widget - INFO - _update_assistant_response: waiting_id=msg_1751913669167, response_type=<class 'str'>, error=False
2025-07-08 00:11:12,264 - gopiai.ui.components.chat_widget - INFO - –¢–µ–∫—É—â–∏–π HTML —Å–æ–¥–µ—Ä–∂–∏—Ç 1285 —Å–∏–º–≤–æ–ª–æ–≤
2025-07-08 00:11:12,265 - gopiai.ui.components.chat_widget - INFO - ‚úÖ –ù–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω: ‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –∑–∞–ø—Ä–æ—Å...
2025-07-08 00:11:12,268 - gopiai.ui.components.chat_widget - INFO - ‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ–∂–∏–¥–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–º–µ–Ω–µ–Ω–æ –Ω–∞ –æ—Ç–≤–µ—Ç
