"""
üåê GopiAI Web Viewer Tool –¥–ª—è CrewAI
–ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
"""

import requests
import logging
from typing import Type, Any, Optional, Dict
from pydantic import BaseModel, Field
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin, urlparse
import re

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º BaseTool –∏–∑ crewai
from crewai.tools.base_tool import BaseTool

class WebViewerInput(BaseModel):
    """–°—Ö–µ–º–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü"""
    action: str = Field(description="–î–µ–π—Å—Ç–≤–∏–µ: fetch, extract_text, extract_links, get_title, get_meta")
    url: str = Field(description="URL –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞")
    selector: str = Field(default="", description="CSS —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
    max_length: int = Field(default=5000, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")

class GopiAIWebViewerTool(BaseTool):
    """
    –ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü
    
    –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
    - –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü —á–µ—Ä–µ–∑ HTTP/HTTPS
    - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    - –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    - –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ HTML
    """
    
    name: str = Field(default="gopiai_web_viewer", description="–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü")
    description: str = Field(default="""–ü—Ä–æ—Å—Ç–æ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü –±–µ–∑ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: –∑–∞–≥—Ä—É–∑–∫—É —Å—Ç—Ä–∞–Ω–∏—Ü, –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, –ø–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫, –ø–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.
    –ë–µ–∑–æ–ø–∞—Å–µ–Ω –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –±—Ä–∞—É–∑–µ—Ä–Ω—ã—Ö –¥—Ä–∞–π–≤–µ—Ä–æ–≤.""", description="–û–ø–∏—Å–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞")
    args_schema: Type[BaseModel] = WebViewerInput
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü"""
        super().__init__()
    
    @property
    def logger(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ª–æ–≥–≥–µ—Ä–∞ –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        return logging.getLogger(__name__)
    
    @property
    def session(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ HTTP —Å–µ—Å—Å–∏–∏"""
        if not hasattr(self, '_session'):
            self._session = requests.Session()
            self._session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
        return self._session
    
    @property
    def timeout(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–∞–π–º–∞—É—Ç–∞"""
        return 10
    
    @property
    def _page_cache(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
        if not hasattr(self, '_cache'):
            self._cache = {}
        return self._cache
    
    def _run(self, action: str, url: str, selector: str = "", max_length: int = 5000) -> str:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è URL
            if not url or not url.strip():
                return "‚ùå URL –Ω–µ —É–∫–∞–∑–∞–Ω"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ç–æ–∫–æ–ª –µ—Å–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç
            if not url.startswith(('http://', 'https://')):
                url = f"https://{url}"
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
            if action == "fetch":
                return self._fetch_page(url)
            elif action == "extract_text":
                return self._extract_text(url, selector, max_length)
            elif action == "extract_links":
                return self._extract_links(url)
            elif action == "get_title":
                return self._get_title(url)
            elif action == "get_meta":
                return self._get_meta(url)
            else:
                return f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: {action}. –î–æ—Å—Ç—É–ø–Ω—ã–µ: fetch, extract_text, extract_links, get_title, get_meta"
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã: {str(e)}"
    
    def _fetch_page(self, url: str) -> str:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            if url in self._page_cache:
                self.logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É: {url}")
                soup = self._page_cache[url]
            else:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                
                # –ü–∞—Ä—Å–∏–º HTML
                soup = BeautifulSoup(response.text, 'html.parser')
                self._page_cache[url] = soup
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = ""
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                description = meta_desc.get('content', '')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤)
            body_text = ""
            body = soup.find('body')
            if body:
                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for script in body(["script", "style"]):
                    script.decompose()
                body_text = body.get_text(separator=' ', strip=True)[:2000]
            
            result = f"""‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {url}
            
üìÑ –ó–∞–≥–æ–ª–æ–≤–æ–∫: {title_text}

üìù –û–ø–∏—Å–∞–Ω–∏–µ: {description}

üìñ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ (–ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤):
{body_text}

üîó URL: {url}
üìä –°—Ç–∞—Ç—É—Å: –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ
"""
            return result
            
        except requests.exceptions.RequestException as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {str(e)}"
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {str(e)}"
    
    def _extract_text(self, url: str, selector: str = "", max_length: int = 5000) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –µ—Å–ª–∏ –Ω–µ –≤ –∫—ç—à–µ
            if url not in self._page_cache:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                self._page_cache[url] = soup
            else:
                soup = self._page_cache[url]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            if selector:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º CSS —Å–µ–ª–µ–∫—Ç–æ—Ä
                elements = soup.select(selector)
                if not elements:
                    return f"‚ùå –≠–ª–µ–º–µ–Ω—Ç—ã —Å —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º '{selector}' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ {url}"
                
                texts = []
                for elem in elements[:10]:  # –ú–∞–∫—Å–∏–º—É–º 10 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                    text = elem.get_text(separator=' ', strip=True)
                    if text:
                        texts.append(text)
                
                result_text = '\n\n'.join(texts)
            else:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
                for script in soup(["script", "style", "nav", "header", "footer"]):
                    script.decompose()
                
                result_text = soup.get_text(separator=' ', strip=True)
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            if len(result_text) > max_length:
                result_text = result_text[:max_length] + "..."
            
            return f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω —Ç–µ–∫—Å—Ç —Å {url} ({len(result_text)} —Å–∏–º–≤–æ–ª–æ–≤):\n\n{result_text}"
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å {url}: {str(e)}"
    
    def _extract_links(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –µ—Å–ª–∏ –Ω–µ –≤ –∫—ç—à–µ
            if url not in self._page_cache:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                self._page_cache[url] = soup
            else:
                soup = self._page_cache[url]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
            links = []
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text(strip=True)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                absolute_url = urljoin(url, href)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ HTTP/HTTPS —Å—Å—ã–ª–∫–∏
                if absolute_url.startswith(('http://', 'https://')):
                    links.append({
                        'url': absolute_url,
                        'text': text[:100] if text else '–ë–µ–∑ —Ç–µ–∫—Å—Ç–∞'
                    })
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Å—ã–ª–æ–∫
            links = links[:20]
            
            result = f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ {url}:\n\n"
            for i, link in enumerate(links, 1):
                result += f"{i}. {link['text']}\n   {link['url']}\n\n"
            
            return result
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ —Å {url}: {str(e)}"
    
    def _get_title(self, url: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –µ—Å–ª–∏ –Ω–µ –≤ –∫—ç—à–µ
            if url not in self._page_cache:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                self._page_cache[url] = soup
            else:
                soup = self._page_cache[url]
            
            title = soup.find('title')
            if title:
                title_text = title.get_text(strip=True)
                return f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {title_text}"
            else:
                return f"‚ùå –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∞ {url}"
                
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å {url}: {str(e)}"
    
    def _get_meta(self, url: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –µ—Å–ª–∏ –Ω–µ –≤ –∫—ç—à–µ
            if url not in self._page_cache:
                response = self.session.get(url, timeout=self.timeout)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                self._page_cache[url] = soup
            else:
                soup = self._page_cache[url]
            
            meta_info = {}
            
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title = soup.find('title')
            if title:
                meta_info['title'] = title.get_text(strip=True)
            
            # –û–ø–∏—Å–∞–Ω–∏–µ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                meta_info['description'] = meta_desc.get('content', '')
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
            if meta_keywords:
                meta_info['keywords'] = meta_keywords.get('content', '')
            
            # Open Graph –¥–∞–Ω–Ω—ã–µ
            og_title = soup.find('meta', attrs={'property': 'og:title'})
            if og_title:
                meta_info['og_title'] = og_title.get('content', '')
            
            og_desc = soup.find('meta', attrs={'property': 'og:description'})
            if og_desc:
                meta_info['og_description'] = og_desc.get('content', '')
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}:\n\n"
            for key, value in meta_info.items():
                result += f"üìã {key}: {value}\n"
            
            return result if meta_info else f"‚ùå –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –Ω–∞ {url}"
            
        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å {url}: {str(e)}"
    
    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à —Å—Ç—Ä–∞–Ω–∏—Ü"""
        self._page_cache.clear()
        return "‚úÖ –ö—ç—à —Å—Ç—Ä–∞–Ω–∏—Ü –æ—á–∏—â–µ–Ω"