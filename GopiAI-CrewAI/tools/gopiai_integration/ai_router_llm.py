"""
ü§ñ AI Router LLM –¥–ª—è CrewAI
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è CrewAI —Å —Å–∏—Å—Ç–µ–º–æ–π AI Router GopiAI
"""

import os
import sys
import json
import subprocess
from typing import Any, Dict, List, Optional
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.outputs import LLMResult

class GopiAIRouterLLM(LLM):
    """
    –ö–∞—Å—Ç–æ–º–Ω—ã–π LLM –¥–ª—è CrewAI, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π AI Router —Å–∏—Å—Ç–µ–º—É GopiAI
    """
    
    ai_router_path: str = "../../01_AI_ROUTER_SYSTEM/ai_router_system.js"
    task_type: str = "chat"
    
    @property
    def _llm_type(self) -> str:
        return "gopiai_router"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ AI Router
        """
        try:
            # –ü—É—Ç—å –∫ AI Router —Å–∏—Å—Ç–µ–º–µ
            router_path = os.path.join(os.path.dirname(__file__), self.ai_router_path)
            
            # –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ AI Router —á–µ—Ä–µ–∑ Node.js
            cmd = [
                "node", 
                "-e", 
                f"""
                const AIRouter = require('{router_path}');
                const config = require('../../01_AI_ROUTER_SYSTEM/ai_rotation_config.js');
                
                async function processRequest() {{
                    const router = new AIRouter(config.AI_PROVIDERS_CONFIG);
                    try {{
                        const result = await router.chat('{prompt.replace("'", "\\'")}', '{self.task_type}');
                        console.log(JSON.stringify({{ success: true, response: result.response }}));
                    }} catch (error) {{
                        console.log(JSON.stringify({{ success: false, error: error.message }}));
                    }}
                }}
                
                processRequest();
                """
            ]
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                cwd=os.path.dirname(__file__)
            )
            
            if result.returncode == 0:
                # –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
                try:
                    response_data = json.loads(result.stdout.strip())
                    if response_data.get("success"):
                        return response_data["response"]
                    else:
                        return f"–û—à–∏–±–∫–∞ AI Router: {response_data.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
                except json.JSONDecodeError:
                    return f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ AI Router: {result.stdout}"
            else:
                return f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è AI Router: {result.stderr}"
                
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å AI Router: {str(e)}"

class GopiAICodingLLM(GopiAIRouterLLM):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM –¥–ª—è –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è"""
    task_type: str = "code"

class GopiAICreativeLLM(GopiAIRouterLLM):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM –¥–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á"""
    task_type: str = "creative"

class GopiAIAnalysisLLM(GopiAIRouterLLM):
    """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π LLM –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á"""
    task_type: str = "analysis"


# üéØ –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —á–µ—Ä–µ–∑ –ø—Ä—è–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–µ–π (fallback)
class SimpleGopiAILLM(LLM):
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è –∫–ª—é—á–∏ –Ω–∞–ø—Ä—è–º—É—é
    """
    
    provider: str = "groq"  # groq, openai, gemini
    
    @property
    def _llm_type(self) -> str:
        return f"gopiai_simple_{self.provider}"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        –ü—Ä–æ—Å—Ç–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        """
        try:
            if self.provider == "groq":
                return self._call_groq(prompt)
            elif self.provider == "openai":
                return self._call_openai(prompt)
            # –î–æ–±–∞–≤–∏—Ç—å –¥—Ä—É–≥–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            else:
                return "–ü—Ä–æ–≤–∞–π–¥–µ—Ä –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è"
                
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ {self.provider}: {str(e)}"
    
    def _call_groq(self, prompt: str) -> str:
        """–í—ã–∑–æ–≤ —á–µ—Ä–µ–∑ Groq API"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=os.getenv("GROQ_API_KEY"),
                base_url="https://api.groq.com/openai/v1"
            )
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ .env
            model = os.getenv("GROQ_MODEL", "llama-3.3-70b-versatile")
            
            response = client.chat.completions.create(
                model=model,  # ‚úÖ –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–∫—Ç—É–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ Groq API: {str(e)}"
    
    def _call_openai(self, prompt: str) -> str:
        """–í—ã–∑–æ–≤ —á–µ—Ä–µ–∑ OpenAI API"""
        try:
            import openai
            client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1000,
                temperature=0.7
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ OpenAI API: {str(e)}"


# üéØ –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è LLM
def create_gopiai_llm(provider: str = "router", task_type: str = "chat"):
    """
    –°–æ–∑–¥–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π LLM –¥–ª—è CrewAI
    
    Args:
        provider: "router" (AI Router), "groq", "openai", "simple"
        task_type: "chat", "code", "creative", "analysis"
    """
    
    if provider == "router":
        if task_type == "code":
            return GopiAICodingLLM()
        elif task_type == "creative":
            return GopiAICreativeLLM()
        elif task_type == "analysis":
            return GopiAIAnalysisLLM()
        else:
            return GopiAIRouterLLM()
    
    elif provider in ["groq", "openai"]:
        return SimpleGopiAILLM(provider=provider)
    
    else:
        # Fallback –∫ Groq
        return SimpleGopiAILLM(provider="groq")


if __name__ == "__main__":
    # –¢–µ—Å—Ç LLM
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ GopiAI LLM...")
    
    # –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–≥–æ LLM
    llm = create_gopiai_llm("groq")
    response = llm("–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?")
    print(f"–û—Ç–≤–µ—Ç Groq: {response}")