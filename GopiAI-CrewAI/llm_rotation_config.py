import os
import time
import threading
# –ö–æ–Ω—Ñ–∏–≥ –º–æ–¥–µ–ª–µ–π Gemini/Gemma –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏ –∏ –∑–∞–¥–∞—á
LLM_MODELS_CONFIG = [
    {
        "name": "Gemini 1.5 Flash",
        "id": "gemini/gemini-1.5-flash",
        "provider": "google",
        "rpm": 15,  
        "tpm": 250000,  
        "type": ["simple", "dialog", "code", "summarize"],
        "multimodal": False,
        "embedding": False,
        "priority": 3,
        "rpd": 50,  
        "deprecated": False,
        "base_score": 0.5
    },
    {
        "name": "Gemini 2.0 Flash-Lite",
        "id": "gemini/gemini-2.0-flash-lite",
        "provider": "google",
        "rpm": 30,
        "tpm": 1000000,  
        "type": ["simple", "dialog", "code", "summarize"],
        "multimodal": False,
        "embedding": False,
        "priority": 4,
        "rpd": 200,  
        "deprecated": False,
        "base_score": 0.5
    },
    {
        "name": "Gemma 3",
        "id": "gemini/gemma-3",
        "provider": "google",
        "rpm": 30,  
        "tpm": 14400,  
        "type": ["simple", "lookup", "short_answer"],
        "multimodal": False,
        "embedding": False,
        "priority": 1,
        "rpd": 0,
        "deprecated": False,
        "base_score": 0.5
    },
    {
        "name": "Gemma 3n",
        "id": "gemini/gemma-3n",
        "provider": "google",
        "rpm": 30,  
        "tpm": 14400,  
        "type": ["simple", "lookup", "short_answer"],
        "multimodal": False,
        "embedding": False,
        "priority": 2,
        "rpd": 0,
        "deprecated": False,
        "base_score": 0.5
    },
    {
        "name": "Gemini 2.5 Flash-Lite Preview",
        "id": "gemini/gemini-2.5-flash-lite-preview",
        "provider": "google",
        "rpm": 15,
        "tpm": 60000,
        "type": ["dialog", "code", "summarize"],
        "multimodal": False,
        "embedding": False,
        "priority": 5,
        "rpd": 0,
        "deprecated": False,
        "base_score": 0.5
    },
    {
        "name": "Gemini 2.5 Flash",
        "id": "gemini/gemini-2.5-flash",
        "provider": "google",
        "rpm": 10,
        "tpm": 60000,
        "type": ["dialog", "code", "multimodal", "vision", "long_answer"],
        "multimodal": True,
        "embedding": False,
        "priority": 6,
        "rpd": 0,
        "deprecated": False,
        "base_score": 0.5
    },
    {
        "name": "Gemini Embedding Experimental",
        "id": "gemini/gemini-embedding-experimental",
        "provider": "google",
        "rpm": 5,
        "tpm": 10000,
        "type": ["embedding"],
        "multimodal": False,
        "embedding": True,
        "priority": 10,
        "rpd": 0,
        "deprecated": False,
        "base_score": 0.5
    }
]
print(f"DEBUG: LLM_MODELS_CONFIG loaded: {LLM_MODELS_CONFIG}")
# Helper to get API key based on provider name
def get_api_key_for_provider(provider_name: str):
    """Gets the API key from environment variables for a given provider."""
    print(f"[DEBUG] Getting API key for provider: {provider_name}")
    
    # Check if we're in test environment
    env_test_suffix = "_TEST" if os.getenv("ENVIRONMENT") == "test" else ""
    print(f"[DEBUG] Environment test suffix: '{env_test_suffix}'")
    
    # Map of provider names to their environment variable names
    key_map = {
        "google": "GEMINI_API_KEY"
    }
    print(f"[DEBUG] Key map: {key_map}")
    
    # Get the base environment variable name for the provider
    env_var_base = key_map.get(provider_name.lower())
    print(f"[DEBUG] Environment variable base for {provider_name}: '{env_var_base}'")
    
    if env_var_base is None:
        print(f"[ERROR] No environment variable mapping found for provider: {provider_name}")
        return None
    
    # Construct the full environment variable name
    env_var = env_var_base + env_test_suffix
    print(f"[DEBUG] Full environment variable name: '{env_var}'")
    
    # Get the API key from environment variables
    api_key = os.getenv(env_var)
    
    # Debug output about the API key
    if api_key:
        print(f"[DEBUG] Successfully retrieved API key for {provider_name}")
        print(f"[DEBUG] API key starts with: {api_key[:5]}...{api_key[-5:] if len(api_key) > 10 else ''}")
    else:
        print(f"[ERROR] Failed to get API key for {provider_name} from environment variable: {env_var}")
        print(f"[DEBUG] Current environment variables: {[k for k in os.environ if 'GEMINI' in k or 'API' in k]}")
    
    return api_key
# üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä –ª–∏–º–∏—Ç–æ–≤ —Å blacklist –º–µ—Ö–∞–Ω–∏–∑–º–æ–º
class RateLimitMonitor:
    def __init__(self, models_config):
        self.models = {m["id"]: m for m in models_config}
        self.usage = {
            m["id"]: {
                "rpm": 0, 
                "tpm": 0, 
                "rpd": 0, 
                "last_reset": time.time(), 
                "last_day_reset": time.time()
            } for m in models_config
        }
        
        # üö® –ù–û–í–û–ï: Blacklist –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.blacklisted_models = {}  # {model_id: expiry_timestamp}
        self.lock = threading.Lock()
        
        print("[OK] RateLimitMonitor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å blacklist –º–µ—Ö–∞–Ω–∏–∑–º–æ–º")
    def _reset_if_needed(self, model_id):
        now = time.time()
        # –°–±—Ä–æ—Å usage –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
        if now - self.usage[model_id]["last_reset"] > 60:
            self.usage[model_id]["rpm"] = 0
            self.usage[model_id]["tpm"] = 0
            self.usage[model_id]["last_reset"] = now
        
        # –°–±—Ä–æ—Å RPD –∫–∞–∂–¥—ã–µ 24 —á–∞—Å–∞
        if now - self.usage[model_id]["last_day_reset"] > 86400:  # 24 —á–∞—Å–∞
            self.usage[model_id]["rpd"] = 0
            self.usage[model_id]["last_day_reset"] = now
    # üö® –ù–û–í–û–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏
    def is_model_blocked(self, model_id):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å –≤—Ä–µ–º–µ–Ω–Ω–æ
        –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –≤—ã–∑—ã–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ –∏–∑–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ self.lock!
        """
        now = time.time()
        if model_id in self.blacklisted_models:
            expiry_time = self.blacklisted_models[model_id]
            if now >= expiry_time:
                # –ú–æ–¥–µ–ª—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞, —É–¥–∞–ª—è–µ–º –∏–∑ blacklist
                del self.blacklisted_models[model_id]
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_id} –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏ —É–¥–∞–ª–µ–Ω–∞ –∏–∑ blacklist")
                return False
            else:
                remaining_time = int(expiry_time - now)
                print(f"üö´ –ú–æ–¥–µ–ª—å {model_id} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –µ—â–µ {remaining_time} —Å–µ–∫—É–Ω–¥")
                return True
        return False

    
    def is_model_blocked_safe(self, model_id):
        """–ü—É–±–ª–∏—á–Ω–∞—è –≤–µ—Ä—Å–∏—è is_model_blocked —Å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π"""
        with self.lock:
            return self.is_model_blocked(model_id)
    # üö® –ù–û–í–û–ï: –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö API
    def mark_model_unavailable(self, model_id, duration=3600):
        """–ü–æ–º–µ—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∫–∞–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—É—é –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)"""
        with self.lock:
            expiry_time = time.time() + duration
            self.blacklisted_models[model_id] = expiry_time
            
            # –¢–∞–∫–∂–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ –∏—Å—á–µ—Ä–ø—ã–≤–∞–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –¥–≤–æ–π–Ω–æ–π –∑–∞—â–∏—Ç—ã
            if model_id in self.models:
                self.usage[model_id]["rpm"] = self.models[model_id]["rpm"]
                self.usage[model_id]["rpd"] = max(self.models[model_id]["rpd"], 1) if self.models[model_id]["rpd"] > 0 else 999
                
            print(f"üö´ –ú–æ–¥–µ–ª—å {model_id} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ {duration} —Å–µ–∫—É–Ω–¥ –¥–æ {time.strftime('%H:%M:%S', time.localtime(expiry_time))}")
    # üö® –ò–°–ü–†–ê–í–õ–ï–ù–û: can_use —Ç–µ–ø–µ—Ä—å —É—á–∏—Ç—ã–≤–∞–µ—Ç blacklist
    def can_use(self, model_id, tokens=0):
        try:
            print(f"[CHECK] –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–¥–µ–ª–∏ {model_id} —Å {tokens} —Ç–æ–∫–µ–Ω–∞–º–∏...")
            
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞–≤–∏—Å–∞–Ω–∏—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º timeout
            with self.lock:
                # –ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –º–æ–¥–µ–ª—å –≤ blacklist?
                if self.is_model_blocked(model_id):
                    print(f"[BLOCKED] –ú–æ–¥–µ–ª—å {model_id} –≤ blacklist")
                    return False
                    
                self._reset_if_needed(model_id)
                model = self.models[model_id]
                usage = self.usage[model_id]
                
                print(f"[STATS] –ú–æ–¥–µ–ª—å {model_id}: —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RPM={usage['rpm']}/{model['rpm']}, TPM={usage['tpm']}/{model['tpm']}, RPD={usage['rpd']}/{model['rpd']}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –ª–∏–º–∏—Ç—ã: RPM, TPM –∏ RPD
                rpm_ok = usage["rpm"] < model["rpm"]
                tpm_ok = usage["tpm"] + tokens < model["tpm"]
                rpd_ok = model["rpd"] == 0 or usage["rpd"] < model["rpd"]  # –ï—Å–ª–∏ RPD=0, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
                
                result = rpm_ok and tpm_ok and rpd_ok
                
                print(f"[OK] –ú–æ–¥–µ–ª—å {model_id}: RPM_OK={rpm_ok}, TPM_OK={tpm_ok}, RPD_OK={rpd_ok} -> RESULT={result}")
                
                if not result:
                    print(f"[WARNING] –ú–æ–¥–µ–ª—å {model_id} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: RPM={usage['rpm']}/{model['rpm']}, TPM={usage['tpm']}/{model['tpm']}, RPD={usage['rpd']}/{model['rpd']}")
                
                return result
        except Exception as e:
            print(f"[ERROR] –û–®–ò–ë–ö–ê –≤ can_use –¥–ª—è –º–æ–¥–µ–ª–∏ {model_id}: {e}")
            import traceback
            traceback.print_exc()
            return False
    def register_use(self, model_id, tokens=0):
        with self.lock:
            self._reset_if_needed(model_id)
            self.usage[model_id]["rpm"] += 1
            self.usage[model_id]["tpm"] += tokens
            self.usage[model_id]["rpd"] += 1  # –¢–∞–∫–∂–µ —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –¥–µ–Ω—å
    def wait_for_slot(self, model_id, tokens=0):
        # –ñ–¥–∞—Ç—å, –ø–æ–∫–∞ –Ω–µ –ø–æ—è–≤–∏—Ç—Å—è —Å–ª–æ—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        while not self.can_use(model_id, tokens):
            time.sleep(1)
    # üö® –ù–û–í–û–ï: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —É—á–µ—Ç–æ–º blacklist
    def get_available_models(self, task_type):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö (–Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö) –º–æ–¥–µ–ª–µ–π –¥–ª—è task_type"""
        available = []
        for model in LLM_MODELS_CONFIG:
            if (task_type in model["type"] and 
                not model.get("deprecated", False) and 
                not self.is_model_blocked_safe(model["id"])):
                available.append(model)
        return available
    # üö® –ù–û–í–û–ï: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ blacklist
    def get_blacklist_status(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ blacklist"""
        with self.lock:
            now = time.time()
            active_blocks = {}
            for model_id, expiry_time in self.blacklisted_models.items():
                if now < expiry_time:
                    remaining = int(expiry_time - now)
                    active_blocks[model_id] = remaining
            return active_blocks
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∞
rate_limit_monitor = RateLimitMonitor(LLM_MODELS_CONFIG)
# üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: select_llm_model_safe —Å —É—á–µ—Ç–æ–º blacklist
def select_llm_model_safe(task_type, tokens=0, intelligence_priority=False, exclude_models=None):
    """
    task_type: —Ç–∏–ø –∑–∞–¥–∞—á–∏
    tokens: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
    intelligence_priority: –µ—Å–ª–∏ True, –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ —Å –≤—ã—Å–æ–∫–∏–º base_score
    exclude_models: —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∫ blacklist)
    """
    exclude_models = exclude_models or []
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º blacklist –∏ exclude_models
    suitable_models = []
    for m in LLM_MODELS_CONFIG:
        if (task_type in m["type"] and 
            not m.get("deprecated", False) and
            m["id"] not in exclude_models and
            not rate_limit_monitor.is_model_blocked_safe(m["id"])):
            suitable_models.append(m)
    
    if not suitable_models:
        print(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è task_type '{task_type}'. Blacklist: {rate_limit_monitor.get_blacklist_status()}")
        return None
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Å —É—á—ë—Ç–æ–º base_score –∏–ª–∏ –æ–±—ã—á–Ω–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    if intelligence_priority:
        suitable_models = sorted(suitable_models, key=lambda m: (-m["base_score"], m["priority"]))
    else:
        suitable_models = sorted(suitable_models, key=lambda m: m["priority"])
    
    print(f"[AVAILABLE] –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è '{task_type}': {[m['id'] for m in suitable_models]}")
    
    # –ü–ï–†–í–´–ô –ü–†–û–•–û–î: –ò—â–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
    for model in suitable_models:
        if rate_limit_monitor.can_use(model["id"], tokens):
            print(f"[SELECTED] AI Router: –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å '{model['id']}' (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç {model['priority']})")
            return model["id"]
    
    # –í–¢–û–†–û–ô –ü–†–û–•–û–î: –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–Ω—è—Ç—ã, –±–µ—Ä–µ–º –º–æ–¥–µ–ª—å —Å –Ω–∞–∏–º–µ–Ω—å—à–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º
    print("‚ö†Ô∏è –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –±–ª–∏–∑–∫–∏ –∫ –ª–∏–º–∏—Ç–∞–º, –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—É—é...")
    best_model = None
    best_score = float('inf')
    
    for model in suitable_models:
        usage = rate_limit_monitor.usage[model["id"]]
        model_config = rate_limit_monitor.models[model["id"]]
        
        # –°—á–∏—Ç–∞–µ–º "–∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ—Å—Ç—å" –∫–∞–∫ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—Ç –ª–∏–º–∏—Ç–æ–≤
        rpm_load = usage["rpm"] / model_config["rpm"]
        tpm_load = (usage["tpm"] + tokens) / model_config["tpm"]
        total_load = rpm_load + tpm_load
        
        if total_load < best_score:
            best_score = total_load
            best_model = model
    
    if best_model:
        print(f"‚ö†Ô∏è AI Router: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤—ã–±—Ä–∞–Ω–∞ –Ω–∞–∏–º–µ–Ω–µ–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å '{best_model['id']}' (–∑–∞–≥—Ä—É–∑–∫–∞: {best_score:.2f})")
        return best_model["id"]
    
    # –¢–†–ï–¢–ò–ô –ü–†–û–•–û–î: –í –∫—Ä–∞–π–Ω–µ–º —Å–ª—É—á–∞–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None –¥–ª—è graceful degradation
    print("üò¥ –í—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏—Å—á–µ—Ä–ø–∞–ª–∏ –ª–∏–º–∏—Ç—ã. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–∂–∏–¥–∞–Ω–∏–µ –∏–ª–∏ fallback.")
    return None
# üö® –ù–û–í–û–ï: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–µ–π –¥–æ—Å—Ç—É–ø–Ω–æ–π –º–æ–¥–µ–ª–∏ (fallback chain)
def get_next_available_model(task_type, current_model_id, tokens=0):
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å –≤ —Ü–µ–ø–æ—á–∫–µ fallback"""
    exclude_models = [current_model_id] if current_model_id else []
    return select_llm_model_safe(task_type, tokens, exclude_models=exclude_models)
# –ë—ã—Å—Ç—Ä–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
def validate_llm_config():
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π LLM"""
    required_keys = ["name", "id", "provider", "rpm", "tpm", "type", "multimodal", 
                     "embedding", "priority", "rpd", "deprecated", "base_score"]
    
    for i, model in enumerate(LLM_MODELS_CONFIG):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–ª—é—á–µ–π
        for key in required_keys:
            assert key in model, f"Model {i}: Missing required key '{key}'"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ª–∏–º–∏—Ç–æ–≤
        assert model["rpm"] > 0, f"Model {model['name']}: RPM must be positive, got {model['rpm']}"
        assert model["tpm"] > 0, f"Model {model['name']}: TPM must be positive, got {model['tpm']}"
        assert model["rpd"] >= 0, f"Model {model['name']}: RPD must be non-negative, got {model['rpd']}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º base_score –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1
        assert 0 <= model["base_score"] <= 1, f"Model {model['name']}: base_score must be in range [0,1], got {model['base_score']}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        assert isinstance(model["deprecated"], bool), f"Model {model['name']}: deprecated must be boolean"
        assert isinstance(model["multimodal"], bool), f"Model {model['name']}: multimodal must be boolean"
        assert isinstance(model["embedding"], bool), f"Model {model['name']}: embedding must be boolean"
        assert isinstance(model["type"], list), f"Model {model['name']}: type must be a list"
        assert len(model["type"]) > 0, f"Model {model['name']}: type list cannot be empty"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å ID
        model_ids = [m["id"] for m in LLM_MODELS_CONFIG]
        assert len(model_ids) == len(set(model_ids)), "Model IDs must be unique"
    
    print("[OK] –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LLM –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ")
# –ó–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥—É–ª—è
validate_llm_config()
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–æ–≤—ã–º–∏ –ø–æ–ª—è–º–∏
def get_active_models():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö (–Ω–µ deprecated) –º–æ–¥–µ–ª–µ–π"""
    return [model for model in LLM_MODELS_CONFIG if not model.get("deprecated", False)]
def get_models_by_intelligence(min_score=0.0):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Å base_score –≤—ã—à–µ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞"""
    return [model for model in LLM_MODELS_CONFIG 
            if model.get("base_score", 0) >= min_score and not model.get("deprecated", False)]
def update_model_deprecated_status(model_id, deprecated=True):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å deprecated –¥–ª—è –º–æ–¥–µ–ª–∏"""
    for model in LLM_MODELS_CONFIG:
        if model["id"] == model_id:
            model["deprecated"] = deprecated
            print(f"üìù –ú–æ–¥–µ–ª—å {model_id} –ø–æ–º–µ—á–µ–Ω–∞ –∫–∞–∫ {'deprecated' if deprecated else 'active'}")
            return True
    return False
def get_model_usage_stats(model_id):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏"""
    if model_id in rate_limit_monitor.usage:
        usage = rate_limit_monitor.usage[model_id]
        model_config = rate_limit_monitor.models[model_id]
        return {
            "rpm_used": usage["rpm"],
            "rpm_limit": model_config["rpm"],
            "tpm_used": usage["tpm"],
            "tpm_limit": model_config["tpm"],
            "rpd_used": usage["rpd"],
            "rpd_limit": model_config["rpd"],
            "base_score": model_config["base_score"],
            "deprecated": model_config["deprecated"],
            "is_blocked": rate_limit_monitor.is_model_blocked_safe(model_id)
        }
    return None
# (–∑–∞–≥–æ—Ç–æ–≤–∫–∞, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —É—á—ë—Ç–æ–º —Ç–µ–∫—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –æ—á–µ—Ä–µ–¥–∏)
def select_llm_model(task_type, current_usage):
    """
    task_type: str (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'simple', 'dialog', 'embedding', 'multimodal')
    current_usage: dict {model_id: {"rpm": int, "tpm": int}}
    """
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–æ—Ç –º–µ–Ω—å—à–µ–≥–æ –∫ –±–æ–ª—å—à–µ–º—É)
    for model in sorted(LLM_MODELS_CONFIG, key=lambda m: m["priority"]):
        if task_type in model["type"]:
            usage = current_usage.get(model["id"], {"rpm": 0, "tpm": 0})
            if usage["rpm"] < model["rpm"] and usage["tpm"] < model["tpm"]:
                return model["id"]
    return None  # –µ—Å–ª–∏ –≤—Å–µ –ª–∏–º–∏—Ç—ã –∏—Å—á–µ—Ä–ø–∞–Ω—ã
# üö® –ù–û–í–û–ï: –£–ª—É—á—à–µ–Ω–Ω—ã–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
def safe_llm_call(prompt, llm_call_func, task_type, tokens=0, max_fallback_attempts=3):
    """–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–∑–æ–≤ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
    attempt = 0
    last_error = None
    used_models = []
    
    while attempt < max_fallback_attempts:
        try:
            # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å, –∏—Å–∫–ª—é—á–∞—è —É–∂–µ –ø–æ–ø—Ä–æ–±–æ–≤–∞–Ω–Ω—ã–µ
            model_id = select_llm_model_safe(task_type, tokens, exclude_models=used_models)
            
            if not model_id:
                print(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø–æ—Å–ª–µ {attempt + 1} –ø–æ–ø—ã—Ç–æ–∫. –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ: {used_models}")
                break
                
            used_models.append(model_id)
            print(f"üîÑ –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}: –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å {model_id}")
            
            # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            rate_limit_monitor.register_use(model_id, tokens)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—ã–∑–æ–≤
            result = llm_call_func(prompt, model=model_id)
            print(f"‚úÖ –£—Å–ø–µ—à–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ {model_id}")
            return result
            
        except Exception as e:
            last_error = e
            error_str = str(e).lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ—à–∏–±–∫–æ–π –ª–∏–º–∏—Ç–æ–≤
            is_quota_error = any(keyword in error_str for keyword in [
                'quota', 'rate limit', 'exceeded', 'resource_exhausted', 
                'too many requests', '429', 'billing'
            ])
            
            if is_quota_error:
                print(f"üö´ –ú–æ–¥–µ–ª—å {model_id} –∏—Å—á–µ—Ä–ø–∞–ª–∞ –ª–∏–º–∏—Ç—ã: {e}")
                rate_limit_monitor.mark_model_unavailable(model_id, duration=3600)  # –ë–ª–æ–∫–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
            else:
                print(f"‚ö†Ô∏è –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ —Å –º–æ–¥–µ–ª—å—é {model_id}: {e}")
                
            attempt += 1
    
    # –ï—Å–ª–∏ –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å
    if last_error:
        raise Exception(f"‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –ø–æ—Å–ª–µ {max_fallback_attempts} –ø–æ–ø—ã—Ç–æ–∫. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
    else:
        raise Exception(f"‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è task_type '{task_type}'")
# –ü—Ä–∏–º–µ—Ä –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ txtai + LLM
# (–ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ txtai, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—É—é LLM)
def rag_answer(query, txtai_index, llm_call_func, llm_model_id=None):
    """
    query: str
    txtai_index: –æ–±—ä–µ–∫—Ç txtai (–∏–ª–∏ API)
    llm_call_func: —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–∑–æ–≤–∞ LLM (–Ω–∞–ø—Ä–∏–º–µ—Ä, llm.call)
    llm_model_id: id –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ None, –≤—ã–±–∏—Ä–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)
    """
    # 1. –ü–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ txtai
    results = txtai_index.search(query, limit=3)
    context = "\n".join([r["text"] for r in results])
    
    # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback
    prompt = f"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —ç—Ç–æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n–í–æ–ø—Ä–æ—Å: {query}"
    
    if llm_model_id:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å
        rate_limit_monitor.register_use(llm_model_id, len(prompt) // 3)
        return llm_call_func(prompt, model=llm_model_id)
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä —Å fallback
        return safe_llm_call(prompt, llm_call_func, "dialog", tokens=len(prompt) // 3)