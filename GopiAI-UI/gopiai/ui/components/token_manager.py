"""
Token Manager –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º Claude
==============================================

–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º
–ø—Ä–∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ —Å Claude AI.
"""

import re
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass


@dataclass
class TokenLimits:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–∏–º–∏—Ç–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è Claude"""
    max_total_context: int = 200000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç Claude Sonnet
    reserve_for_response: int = 20000  # –†–µ–∑–µ—Ä–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ Claude
    reserve_for_complex_tasks: int = 50000  # –†–µ–∑–µ—Ä–≤ –¥–ª—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –∑–∞–¥–∞—á/–∫–æ–¥–∞
    max_memory_tokens: int = 130000  # –î–æ—Å—Ç—É–ø–Ω–æ –¥–ª—è –ø–∞–º—è—Ç–∏ (200k - 20k - 50k)
    max_recent_messages_tokens: int = 30000  # –î–ª—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    max_rag_search_tokens: int = 100000  # –î–ª—è RAG –ø–æ–∏—Å–∫–∞


class TokenManager:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º Claude
    """
    
    def __init__(self, limits: TokenLimits = None):
        self.limits = limits or TokenLimits()
        
    def estimate_tokens(self, text: str) -> int:
        """
        –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ.
        –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É: ~4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ/–∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
        """
        if not text:
            return 0
        
        # –ë–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        char_count = len(text)
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if self._is_code_heavy(text):
            # –ö–æ–¥ –æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ —Ç–æ–∫–µ–Ω-–ø–ª–æ—Ç–Ω—ã–π
            estimated_tokens = int(char_count / 3.5)
        elif self._is_russian_heavy(text):
            # –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–Ω–µ–µ —Ç–æ–∫–µ–Ω-–ø–ª–æ—Ç–Ω—ã–π
            estimated_tokens = int(char_count / 4.5)
        else:
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç (–±–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞)
            estimated_tokens = int(char_count / 4.0)
        
        return max(estimated_tokens, 1)  # –ú–∏–Ω–∏–º—É–º 1 —Ç–æ–∫–µ–Ω
    
    def _is_code_heavy(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∫–æ–¥–∞"""
        # –ò—â–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ–¥–∞
        code_indicators = [
            r'def\s+\w+\(',  # Python —Ñ—É–Ω–∫—Ü–∏–∏
            r'class\s+\w+',  # Python –∫–ª–∞—Å—Å—ã
            r'import\s+\w+',  # –ò–º–ø–æ—Ä—Ç—ã
            r'function\s+\w+\(',  # JS —Ñ—É–Ω–∫—Ü–∏–∏
            r'const\s+\w+\s*=',  # JS –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
            r'{\s*\n.*}',  # –ë–ª–æ–∫–∏ –∫–æ–¥–∞
            r'<\w+[^>]*>',  # HTML —Ç–µ–≥–∏
            r'@\w+\(',  # –î–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã/–∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
        ]
        
        code_matches = sum(1 for pattern in code_indicators if re.search(pattern, text, re.MULTILINE))
        total_lines = len(text.split('\n'))
        
        return code_matches > 0 and (code_matches / max(total_lines, 1)) > 0.1
    
    def _is_russian_heavy(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä—É—Å—Å–∫–æ–≥–æ"""
        # –ü–æ–¥—Å—á–µ—Ç —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        russian_chars = len(re.findall(r'[–∞-—è—ë]', text.lower()))
        total_chars = len(text)
        
        return total_chars > 0 and (russian_chars / total_chars) > 0.3
    
    def optimize_recent_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–µ—â–∞—é—Ç—Å—è –≤ –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤.
        """
        if not messages:
            return []
        
        optimized_messages = []
        total_tokens = 0
        
        # –ò–¥–µ–º —Å –∫–æ–Ω—Ü–∞ (—Å–∞–º—ã–µ —Å–≤–µ–∂–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
        for message in reversed(messages):
            content = message.get('content', '')
            message_tokens = self.estimate_tokens(content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ
            if total_tokens + message_tokens <= self.limits.max_recent_messages_tokens:
                optimized_messages.insert(0, message)  # –í—Å—Ç–∞–≤–ª—è–µ–º –≤ –Ω–∞—á–∞–ª–æ
                total_tokens += message_tokens
            else:
                # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ, –ø—ã—Ç–∞–µ–º—Å—è –µ–≥–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å
                if len(optimized_messages) == 0:  # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                    truncated_content = self._truncate_to_token_limit(
                        content, 
                        self.limits.max_recent_messages_tokens // 2
                    )
                    if truncated_content:
                        truncated_message = message.copy()
                        truncated_message['content'] = truncated_content + "... [—Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–æ]"
                        optimized_messages.insert(0, truncated_message)
                break
        
        return optimized_messages
    
    def _truncate_to_token_limit(self, text: str, token_limit: int) -> str:
        """–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–æ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        if self.estimate_tokens(text) <= token_limit:
            return text
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –¥–ª–∏–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        estimated_char_limit = token_limit * 4
        
        if len(text) <= estimated_char_limit:
            return text
        
        # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
        sentences = re.split(r'[.!?]\s+', text)
        
        result = ""
        for sentence in sentences:
            test_result = result + sentence + ". "
            if self.estimate_tokens(test_result) <= token_limit:
                result = test_result
            else:
                break
        
        if result:
            return result.strip()
        
        # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º, –æ–±—Ä–µ–∑–∞–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º
        return text[:estimated_char_limit]
    
    def build_enhanced_context(self, 
                             current_message: str,
                             recent_messages: List[Dict[str, Any]] = None,
                             rag_results: List[Dict[str, Any]] = None) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±–æ–≥–∞—â–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è Claude —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–æ–≤.
        """
        context_parts = []
        total_tokens = 0
        
        # –û—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        current_tokens = self.estimate_tokens(current_message)
        
        # 1. –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ)
        if recent_messages:
            optimized_recent = self.optimize_recent_messages(recent_messages)
            if optimized_recent:
                recent_context = self._format_recent_messages(optimized_recent)
                recent_tokens = self.estimate_tokens(recent_context)
                
                if total_tokens + recent_tokens <= self.limits.max_memory_tokens:
                    context_parts.append("## üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π:")
                    context_parts.append(recent_context)
                    total_tokens += recent_tokens
        
        # 2. –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã RAG –ø–æ–∏—Å–∫–∞ (–æ—Å—Ç–∞–≤—à–µ–µ—Å—è –º–µ—Å—Ç–æ)
        if rag_results:
            remaining_tokens = self.limits.max_memory_tokens - total_tokens
            rag_context = self._format_rag_results(rag_results, remaining_tokens)
            if rag_context:
                context_parts.append("## üß† –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ø–∞–º—è—Ç–∏:")
                context_parts.append(rag_context)
        
        # 3. –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        context_parts.append("## üí¨ –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å:")
        context_parts.append(current_message)
        
        return "\n\n".join(context_parts)
    
    def _format_recent_messages(self, messages: List[Dict[str, Any]]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        formatted = []
        
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            timestamp = msg.get('timestamp', '')
            
            role_emoji = {
                'user': 'üë§',
                'assistant': 'ü§ñ',
                'system': '‚öôÔ∏è'
            }.get(role, 'üí¨')
            
            if timestamp:
                formatted.append(f"{role_emoji} **{role.title()}** ({timestamp}):")
            else:
                formatted.append(f"{role_emoji} **{role.title()}**:")
            
            formatted.append(content)
            formatted.append("")  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        
        return "\n".join(formatted)
    
    def _format_rag_results(self, results: List[Dict[str, Any]], token_limit: int) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ RAG –ø–æ–∏—Å–∫–∞ —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        if not results:
            return ""
        
        formatted_parts = []
        used_tokens = 0
        
        for i, result in enumerate(results):
            title = result.get('title', f'–†–µ–∑—É–ª—å—Ç–∞—Ç {i+1}')
            content = result.get('context_preview', result.get('matched_content', ''))
            relevance = result.get('relevance_score', 0.0)
            timestamp = result.get('timestamp', '')
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result_text = f"### üìã {title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.1%})\n"
            if timestamp:
                result_text += f"*–î–∞—Ç–∞: {timestamp}*\n\n"
            result_text += content + "\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result_tokens = self.estimate_tokens(result_text)
            
            if used_tokens + result_tokens <= token_limit:
                formatted_parts.append(result_text)
                used_tokens += result_tokens
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if len(formatted_parts) == 0:  # –ï—Å–ª–∏ —ç—Ç–æ –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    available_tokens = token_limit - used_tokens
                    truncated_content = self._truncate_to_token_limit(content, available_tokens - 100)
                    if truncated_content:
                        result_text = f"### üìã {title} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance:.1%})\n"
                        if timestamp:
                            result_text += f"*–î–∞—Ç–∞: {timestamp}*\n\n"
                        result_text += truncated_content + "... [—Å–æ–∫—Ä–∞—â–µ–Ω–æ]"
                        formatted_parts.append(result_text)
                break
        
        return "\n".join(formatted_parts) if formatted_parts else ""
    
    def get_token_usage_stats(self, context: str) -> Dict[str, int]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤"""
        total_tokens = self.estimate_tokens(context)
        
        return {
            "total_tokens": total_tokens,
            "max_context": self.limits.max_total_context,
            "available_for_response": self.limits.reserve_for_response,
            "available_for_tasks": self.limits.reserve_for_complex_tasks,
            "usage_percentage": round((total_tokens / self.limits.max_total_context) * 100, 1)
        }