#!/usr/bin/env python3
"""
GopiAI Project Health Checker üè•
================================

–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è –ø—Ä–æ–µ–∫—Ç–∞ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏.
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ —É—Ç–∏–ª–∏—Ç—ã –æ—á–∏—Å—Ç–∫–∏, –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ.

–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:
- üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –ø–∞–ø–æ–∫
- üßπ –ü–æ–∏—Å–∫ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤
- üíÄ –ü–æ–∏—Å–∫ –º—ë—Ä—Ç–≤–æ–≥–æ –∫–æ–¥–∞
- üìä –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
- üîç –ü–æ–∏—Å–∫ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
- üìù –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤
- üîó –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- üìà –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–æ–≤

–ê–≤—Ç–æ—Ä: Crazy Coder
–î–∞—Ç–∞: 2025-06-05
"""

import os
import sys
import json
import shutil
import hashlib
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set, Tuple, Optional
import argparse
import subprocess
from collections import defaultdict
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)

class ProjectHealthChecker:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∑–¥–æ—Ä–æ–≤—å—è –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self, project_root: Optional[Path] = None):
        self.project_root = project_root or Path(__file__).parent.parent
        self.reports_dir = self.project_root / "project_health" / "reports"
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # –ü–∞–ø–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å
        self.ignore_dirs = {
            '__pycache__', '.git', 'node_modules', 'venv', 'env', '.venv',
            'rag_memory_env', '.pytest_cache', 'dist', 'build', '.egg-info',
            '.mypy_cache', '.tox', 'logs'
        }
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.code_extensions = {'.py', '.js', '.ts', '.jsx', '.tsx', '.vue', '.css', '.html'}
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–æ–∫
        self.results = {
            'empty_files': [],
            'empty_dirs': [],
            'duplicate_files': [],
            'unused_files': [],
            'dead_code': [],
            'import_issues': [],
            'large_files': [],
            'suspicious_files': []
        }
    
    def run_full_check(self) -> Dict:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞"""
        print("üè• GopiAI Project Health Checker")
        print("=" * 50)
        print(f"üìÅ –ü—Ä–æ–µ–∫—Ç: {self.project_root}")
        print(f"üìä –û—Ç—á—ë—Ç—ã: {self.reports_dir}")
        print()
        
        checks = [
            ("üóëÔ∏è  –ü–æ–∏—Å–∫ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤", self.find_empty_files),
            ("üìÇ –ü–æ–∏—Å–∫ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫", self.find_empty_directories),
            ("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤", self.find_duplicate_files),
            ("üíÄ –ê–Ω–∞–ª–∏–∑ –º—ë—Ä—Ç–≤–æ–≥–æ –∫–æ–¥–∞", self.analyze_dead_code),
            ("üîó –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤", self.check_imports),
            ("üìè –ü–æ–∏—Å–∫ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤", self.find_large_files),
            ("‚ö†Ô∏è  –ü–æ–∏—Å–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤", self.find_suspicious_files),
            ("üìù –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤", self.normalize_filenames),
        ]
        
        for name, check_func in checks:
            print(f"{name}...")
            try:
                check_func()
                print(f"   ‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ")
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
                logger.error(f"–û—à–∏–±–∫–∞ –≤ {name}: {e}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á—ë—Ç
        self.generate_report()
        
        # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—á–∏—Å—Ç–∫—É
        self.suggest_cleanup()
        
        return self.results
    
    def find_empty_files(self):
        """–ü–æ–∏—Å–∫ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        empty_files = []
        
        for file_path in self.project_root.rglob('*'):
            if (file_path.is_file() and 
                not any(ignore in file_path.parts for ignore in self.ignore_dirs)):
                
                try:
                    if file_path.stat().st_size == 0:
                        empty_files.append(str(file_path.relative_to(self.project_root)))
                    elif file_path.suffix == '.py':
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Python —Ñ–∞–π–ª—ã –Ω–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                        content = file_path.read_text(encoding='utf-8', errors='ignore').strip()
                        if not content or content in ['', '#!/usr/bin/env python3', '# -*- coding: utf-8 -*-']:
                            empty_files.append(str(file_path.relative_to(self.project_root)))
                except (OSError, UnicodeDecodeError):
                    continue
        
        self.results['empty_files'] = empty_files
        print(f"   üìÑ –ù–∞–π–¥–µ–Ω–æ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(empty_files)}")
    
    def find_empty_directories(self):
        """–ü–æ–∏—Å–∫ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫"""
        empty_dirs = []
        
        for dir_path in self.project_root.rglob('*'):
            if (dir_path.is_dir() and 
                not any(ignore in dir_path.parts for ignore in self.ignore_dirs)):
                
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ)
                    has_files = any(p.is_file() for p in dir_path.rglob('*'))
                    if not has_files:
                        empty_dirs.append(str(dir_path.relative_to(self.project_root)))
                except OSError:
                    continue
        
        self.results['empty_dirs'] = empty_dirs
        print(f"   üìÇ –ù–∞–π–¥–µ–Ω–æ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫: {len(empty_dirs)}")
    
    def find_duplicate_files(self):
        """–ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É"""
        file_hashes = defaultdict(list)
        duplicates = []
        
        for file_path in self.project_root.rglob('*'):
            if (file_path.is_file() and 
                file_path.suffix in self.code_extensions and
                not any(ignore in file_path.parts for ignore in self.ignore_dirs)):
                
                try:
                    content = file_path.read_bytes()
                    if len(content) > 0:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
                        file_hash = hashlib.md5(content).hexdigest()
                        file_hashes[file_hash].append(str(file_path.relative_to(self.project_root)))
                except (OSError, UnicodeDecodeError):
                    continue
        
        # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        for hash_value, files in file_hashes.items():
            if len(files) > 1:
                duplicates.append(files)
        
        self.results['duplicate_files'] = duplicates
        print(f"   üîç –ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(duplicates)}")
    
    def analyze_dead_code(self):
        """–ê–Ω–∞–ª–∏–∑ –º—ë—Ä—Ç–≤–æ–≥–æ –∫–æ–¥–∞"""
        dead_code_files = []
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑: —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è
        all_python_files = set()
        imported_modules = set()
        
        for file_path in self.project_root.rglob('*.py'):
            if not any(ignore in file_path.parts for ignore in self.ignore_dirs):
                all_python_files.add(str(file_path.relative_to(self.project_root)))
                
                try:
                    content = file_path.read_text(encoding='utf-8', errors='ignore')
                    # –ò—â–µ–º –∏–º–ø–æ—Ä—Ç—ã
                    import_patterns = [
                        r'from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import',
                        r'import\s+([a-zA-Z_][a-zA-Z0-9_.]*)',
                    ]
                    
                    for pattern in import_patterns:
                        matches = re.findall(pattern, content)
                        for match in matches:
                            if '.' in match:
                                # –ú–æ–¥—É–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç
                                parts = match.split('.')
                                for i in range(len(parts)):
                                    module_path = '/'.join(parts[:i+1]) + '.py'
                                    imported_modules.add(module_path)
                            else:
                                imported_modules.add(f"{match}.py")
                
                except (OSError, UnicodeDecodeError):
                    continue
        
        # –§–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è (–≤–æ–∑–º–æ–∂–Ω–æ –º—ë—Ä—Ç–≤—ã–π –∫–æ–¥)
        potentially_dead = all_python_files - imported_modules
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã
        main_files = {'main.py', '__main__.py', 'setup.py', 'run.py'}
        for file_path in potentially_dead:
            if (Path(file_path).name not in main_files and 
                not file_path.endswith('__init__.py') and
                'test' not in file_path.lower()):
                dead_code_files.append(file_path)
        
        self.results['dead_code'] = dead_code_files
        print(f"   üíÄ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º—ë—Ä—Ç–≤—ã–π –∫–æ–¥: {len(dead_code_files)} —Ñ–∞–π–ª–æ–≤")
    
    def check_imports(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–±–ª–µ–º —Å –∏–º–ø–æ—Ä—Ç–∞–º–∏"""
        import_issues = []
        
        for file_path in self.project_root.rglob('*.py'):
            if not any(ignore in file_path.parts for ignore in self.ignore_dirs):
                try:
                    content = file_path.read_text(encoding='utf-8', errors='ignore')
                    lines = content.split('\n')
                    
                    for line_num, line in enumerate(lines, 1):
                        line = line.strip()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã
                        if line.startswith('import ') or line.startswith('from '):
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–≤—ë–∑–¥–æ—á–∫–∏
                            if 'import *' in line:
                                import_issues.append({
                                    'file': str(file_path.relative_to(self.project_root)),
                                    'line': line_num,
                                    'issue': 'star_import',
                                    'content': line
                                })
                            
                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏–º–ø–æ—Ä—Ç–∞
                            if len(line) > 100:
                                import_issues.append({
                                    'file': str(file_path.relative_to(self.project_root)),
                                    'line': line_num,
                                    'issue': 'long_import',
                                    'content': line[:100] + '...'
                                })
                
                except (OSError, UnicodeDecodeError):
                    continue
        
        self.results['import_issues'] = import_issues
        print(f"   üîó –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º —Å –∏–º–ø–æ—Ä—Ç–∞–º–∏: {len(import_issues)}")
    
    def find_large_files(self):
        """–ü–æ–∏—Å–∫ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤"""
        large_files = []
        max_size = 1024 * 1024  # 1MB
        
        for file_path in self.project_root.rglob('*'):
            if (file_path.is_file() and 
                not any(ignore in file_path.parts for ignore in self.ignore_dirs)):
                
                try:
                    size = file_path.stat().st_size
                    if size > max_size:
                        large_files.append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'size': size,
                            'size_mb': round(size / (1024 * 1024), 2)
                        })
                except OSError:
                    continue
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É
        large_files.sort(key=lambda x: x['size'], reverse=True)
        
        self.results['large_files'] = large_files
        print(f"   üìè –ù–∞–π–¥–µ–Ω–æ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (>1MB): {len(large_files)}")
    
    def find_suspicious_files(self):
        """–ü–æ–∏—Å–∫ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        suspicious = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        suspicious_patterns = [
            r'.*\.bak$', r'.*\.tmp$', r'.*\.temp$', r'.*~$',
            r'.*\.orig$', r'.*\.old$', r'.*_backup.*',
            r'.*_copy.*', r'.*_test_.*', r'.*\.pyc$',
            r'.*\.log$', r'.*\.cache$'
        ]
        
        for file_path in self.project_root.rglob('*'):
            if (file_path.is_file() and 
                not any(ignore in file_path.parts for ignore in self.ignore_dirs)):
                
                file_str = str(file_path.relative_to(self.project_root))
                
                for pattern in suspicious_patterns:
                    if re.match(pattern, file_str, re.IGNORECASE):
                        suspicious.append({
                            'file': file_str,
                            'reason': f'Matches pattern: {pattern}'
                        })
                        break
        
        self.results['suspicious_files'] = suspicious
        print(f"   ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(suspicious)}")
    
    def normalize_filenames(self):
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–º—ë–Ω —Ñ–∞–π–ª–æ–≤"""
        problematic_files = []
        
        for file_path in self.project_root.rglob('*'):
            if not any(ignore in file_path.parts for ignore in self.ignore_dirs):
                name = file_path.name
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                if re.search(r'[^\w\-_.]', name):
                    problematic_files.append({
                        'file': str(file_path.relative_to(self.project_root)),
                        'issue': 'special_characters'
                    })
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
                if ' ' in name:
                    problematic_files.append({
                        'file': str(file_path.relative_to(self.project_root)),
                        'issue': 'spaces_in_name'
                    })
        
        self.results['problematic_filenames'] = problematic_files
        print(f"   üìù –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ —Å –ø—Ä–æ–±–ª–µ–º–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏: {len(problematic_files)}")
    
    def generate_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–¥—Ä–æ–±–Ω–æ–≥–æ –æ—Ç—á—ë—Ç–∞"""
        timestamp = datetime.now().isoformat()
        
        report = {
            'timestamp': timestamp,
            'project_root': str(self.project_root),
            'summary': {
                'empty_files': len(self.results['empty_files']),
                'empty_dirs': len(self.results['empty_dirs']),
                'duplicate_groups': len(self.results['duplicate_files']),
                'dead_code_files': len(self.results['dead_code']),
                'import_issues': len(self.results['import_issues']),
                'large_files': len(self.results['large_files']),
                'suspicious_files': len(self.results['suspicious_files'])
            },
            'details': self.results
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON –æ—Ç—á—ë—Ç
        report_file = self.reports_dir / f"health_check_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # –°–æ–∑–¥–∞—ë–º —á–∏—Ç–∞–µ–º—ã–π –æ—Ç—á—ë—Ç
        readable_report = self.reports_dir / f"health_check_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        self.create_readable_report(readable_report, report)
        
        print(f"\nüìä –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω:")
        print(f"   üìÑ JSON: {report_file}")
        print(f"   üìù Markdown: {readable_report}")
    
    def create_readable_report(self, file_path: Path, report: Dict):
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∏—Ç–∞–µ–º–æ–≥–æ –æ—Ç—á—ë—Ç–∞ –≤ Markdown"""
        content = f"""# GopiAI Project Health Report

**–î–∞—Ç–∞:** {report['timestamp']}  
**–ü—Ä–æ–µ–∫—Ç:** {report['project_root']}

## üìä –°–≤–æ–¥–∫–∞

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ |
|-----------|------------|
| üóëÔ∏è –ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã | {report['summary']['empty_files']} |
| üìÇ –ü—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏ | {report['summary']['empty_dirs']} |
| üîç –ì—Ä—É–ø–ø—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ | {report['summary']['duplicate_groups']} |
| üíÄ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º—ë—Ä—Ç–≤—ã–π –∫–æ–¥ | {report['summary']['dead_code_files']} |
| üîó –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–º–ø–æ—Ä—Ç–∞–º–∏ | {report['summary']['import_issues']} |
| üìè –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã (>1MB) | {report['summary']['large_files']} |
| ‚ö†Ô∏è –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã | {report['summary']['suspicious_files']} |

## üóëÔ∏è –ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã

"""
        
        if report['details']['empty_files']:
            for file in report['details']['empty_files']:
                content += f"- `{file}`\n"
        else:
            content += "–ü—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚úÖ\n"
        
        content += "\n## üìÇ –ü—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏\n\n"
        if report['details']['empty_dirs']:
            for dir in report['details']['empty_dirs']:
                content += f"- `{dir}/`\n"
        else:
            content += "–ü—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚úÖ\n"
        
        content += "\n## üîç –î—É–±–ª–∏—Ä—É—é—â–∏–µ —Ñ–∞–π–ª—ã\n\n"
        if report['details']['duplicate_files']:
            for i, group in enumerate(report['details']['duplicate_files'], 1):
                content += f"### –ì—Ä—É–ø–ø–∞ {i}\n"
                for file in group:
                    content += f"- `{file}`\n"
                content += "\n"
        else:
            content += "–î—É–±–ª–∏–∫–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã ‚úÖ\n"
        
        content += "\n## üíÄ –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –º—ë—Ä—Ç–≤—ã–π –∫–æ–¥\n\n"
        if report['details']['dead_code']:
            for file in report['details']['dead_code']:
                content += f"- `{file}`\n"
        else:
            content += "–ú—ë—Ä—Ç–≤—ã–π –∫–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚úÖ\n"
        
        content += "\n## üìè –ë–æ–ª—å—à–∏–µ —Ñ–∞–π–ª—ã\n\n"
        if report['details']['large_files']:
            for file_info in report['details']['large_files']:
                content += f"- `{file_info['file']}` - {file_info['size_mb']} MB\n"
        else:
            content += "–ë–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚úÖ\n"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
    
    def suggest_cleanup(self):
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏"""
        total_issues = (
            len(self.results['empty_files']) +
            len(self.results['empty_dirs']) +
            len(self.results['suspicious_files'])
        )
        
        if total_issues > 0:
            print(f"\nüßπ –ù–∞–π–¥–µ–Ω–æ {total_issues} –ø—Ä–æ–±–ª–µ–º, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
            print("–•–æ—Ç–∏—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –æ—á–∏—Å—Ç–∫—É? (y/N): ", end="")
            
            try:
                response = input().strip().lower()
                if response in ['y', 'yes', '–¥–∞']:
                    self.auto_cleanup()
            except (EOFError, KeyboardInterrupt):
                print("\n–û—Ç–º–µ–Ω–∞.")
    
    def auto_cleanup(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞"""
        print("\nüßπ –ó–∞–ø—É—Å–∫ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–∏...")
        
        cleaned_files = 0
        cleaned_dirs = 0
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤
        for file_path in self.results['empty_files']:
            full_path = self.project_root / file_path
            try:
                full_path.unlink()
                cleaned_files += 1
                print(f"   üóëÔ∏è –£–¥–∞–ª—ë–Ω –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path}")
            except OSError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫
        for dir_path in sorted(self.results['empty_dirs'], reverse=True):  # –°–Ω–∞—á–∞–ª–∞ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ
            full_path = self.project_root / dir_path
            try:
                full_path.rmdir()
                cleaned_dirs += 1
                print(f"   üìÇ –£–¥–∞–ª–µ–Ω–∞ –ø—É—Å—Ç–∞—è –ø–∞–ø–∫–∞: {dir_path}")
            except OSError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –ø–∞–ø–∫–∏ {dir_path}: {e}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        for file_info in self.results['suspicious_files']:
            file_path = file_info['file']
            full_path = self.project_root / file_path
            try:
                full_path.unlink()
                cleaned_files += 1
                print(f"   ‚ö†Ô∏è –£–¥–∞–ª—ë–Ω –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª: {file_path}")
            except OSError as e:
                logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file_path}: {e}")
        
        print(f"\n‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"   üóëÔ∏è –£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {cleaned_files}")
        print(f"   üìÇ –£–¥–∞–ª–µ–Ω–æ –ø–∞–ø–æ–∫: {cleaned_dirs}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—Ä—Ç—É –ø—Ä–æ–µ–∫—Ç–∞
        print("\nüîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞...")
        try:
            subprocess.run([
                sys.executable, 
                str(self.project_root / "update_project_map.py")
            ], cwd=self.project_root)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–∞—Ä—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞: {e}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description="GopiAI Project Health Checker")
    parser.add_argument('--project-root', type=Path, help='–ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞')
    parser.add_argument('--auto-clean', action='store_true', help='–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –±–µ–∑ –∑–∞–ø—Ä–æ—Å–∞')
    parser.add_argument('--report-only', action='store_true', help='–¢–æ–ª—å–∫–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞')
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä checker'–∞
    checker = ProjectHealthChecker(args.project_root)
    
    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É
        results = checker.run_full_check()
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –µ—Å–ª–∏ –∑–∞–ø—Ä–æ—à–µ–Ω–∞
        if args.auto_clean and not args.report_only:
            checker.auto_cleanup()
        
        return 0
        
    except KeyboardInterrupt:
        print("\n‚ùå –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
